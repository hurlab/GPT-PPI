{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiN4y6gRVpSp"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install keras==2.9.0\n",
        "!pip install Keras-Preprocessing==1.1.2\n",
        "\n",
        "!pip install sacremoses\n",
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rp39FUyOUvk_"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import time\n",
        "import torch\n",
        "\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kG3lCSYVgHE"
      },
      "outputs": [],
      "source": [
        "### HELPER METHODS\n",
        "###################\n",
        "\n",
        "# Function to convert seconds to datetime format hh:mm:ss\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def prepare_input_ids_and_attention_masks(tokenizer, sentences, max_len):\n",
        "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "    input_ids = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for sent in sentences:\n",
        "        encoded_sent = tokenizer.encode(\n",
        "                            sent,\n",
        "                            add_special_tokens = True,\n",
        "                    )\n",
        "\n",
        "        input_ids.append(encoded_sent)\n",
        "\n",
        "    # Pad our input tokens\n",
        "    input_ids = pad_sequences(input_ids, maxlen=max_len,\n",
        "                            dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "    # Create attention masks\n",
        "    attention_masks = []\n",
        "\n",
        "    # Create a mask of 1s for each token followed by 0s for padding\n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "    return input_ids, attention_masks\n",
        "\n",
        "def prepare_data_loaders(input_ids, attention_masks, labels, batch_size):\n",
        "    # Convert to tensors.\n",
        "    data_inputs = torch.tensor(input_ids)\n",
        "    data_masks = torch.tensor(attention_masks)\n",
        "    data_labels = torch.tensor(labels)\n",
        "    # Create the DataLoader.\n",
        "    tensor_data = TensorDataset(data_inputs, data_masks, data_labels)\n",
        "    data_sampler = SequentialSampler(tensor_data)\n",
        "    data_loader = DataLoader(tensor_data, sampler=data_sampler, batch_size=batch_size)\n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKReS8aKwLsj"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForMaskedLM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel"
      ],
      "metadata": {
        "id": "tWHBMAQy6jLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QK2i1rJJVmGO"
      },
      "outputs": [],
      "source": [
        "def train_model(sentences, labels, fold_num, annotate_ino_terms, add_ino_ids_to_tokenizer):\n",
        "\n",
        "    ### LOAD  PRETRAINED PUBMEDBERT MODEL & TOKENIZER\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\")\n",
        "\n",
        "    # with epoch = 6\n",
        "    ### HYPER-PARAMETERS\n",
        "    ####################\n",
        "    MAX_LEN = 256\n",
        "    BATCH_SIZE = 4\n",
        "    EPOCHS = 8\n",
        "    optimizer = AdamW(model.parameters(),\n",
        "                      lr = 2e-5,\n",
        "                      eps = 1e-8,\n",
        "                      weight_decay=0.01\n",
        "                    )\n",
        "\n",
        "    ### USE CUDA AND GPU.\n",
        "    ####################\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "        print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "    else:\n",
        "        print('No GPU available, using the CPU instead.')\n",
        "        device = torch.device(\"cpu\")\n",
        "    model.to(device)\n",
        "    ####################\n",
        "\n",
        "    ### PREPARE DATALOADERS\n",
        "    ####################\n",
        "    input_ids, attention_masks = prepare_input_ids_and_attention_masks(tokenizer, sentences, MAX_LEN)\n",
        "\n",
        "    # Use 90% for training and 10% for validation.\n",
        "    train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels,\n",
        "                                                                random_state=2018, test_size=0.1)\n",
        "    # Do the same for the masks.\n",
        "    train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=2018, test_size=0.1)\n",
        "\n",
        "    train_dataloader = prepare_data_loaders(train_inputs, train_masks, train_labels, BATCH_SIZE)\n",
        "    validation_dataloader = prepare_data_loaders(validation_inputs, validation_masks, validation_labels, BATCH_SIZE)\n",
        "    #########################\n",
        "\n",
        "\n",
        "    # Total number of training steps is number of batches * number of epochs.\n",
        "    total_steps = len(train_dataloader) * EPOCHS\n",
        "\n",
        "    # Create the learning rate scheduler.\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                                num_training_steps = total_steps)\n",
        "\n",
        "    # Set the seed value all over the place to make this reproducible.\n",
        "    seed_val = 42\n",
        "\n",
        "    random.seed(seed_val)\n",
        "    np.random.seed(seed_val)\n",
        "    torch.manual_seed(seed_val)\n",
        "    torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "    # Store the average loss after each epoch so we can plot them.\n",
        "    loss_values = []\n",
        "\n",
        "    # For each epoch...\n",
        "    for epoch_i in range(0, EPOCHS):\n",
        "\n",
        "        # ========================================\n",
        "        #               Training\n",
        "        # ========================================\n",
        "\n",
        "        # Perform one full pass over the training set.\n",
        "\n",
        "        print(\"\")\n",
        "        print('======== Fold {:} - Epoch {:} / {:} ========'.format(fold_num, epoch_i + 1, EPOCHS))\n",
        "        print('Training...')\n",
        "\n",
        "        # Measure how long the training epoch takes.\n",
        "        t0 = time.time()\n",
        "\n",
        "        # Reset the total loss for this epoch.\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "            # Progress update every 40 batches.\n",
        "            if step % 40 == 0 and not step == 0:\n",
        "                # Calculate elapsed time in minutes.\n",
        "                elapsed = format_time(time.time() - t0)\n",
        "\n",
        "                # Report progress.\n",
        "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "            # Unpack this training batch from our dataloader.\n",
        "            #\n",
        "            # As we unpack the batch, we'll also copy each tensor to the GPU using the\n",
        "            # `to` method.\n",
        "            #\n",
        "            # `batch` contains three pytorch tensors:\n",
        "            #   [0]: input ids\n",
        "            #   [1]: attention masks\n",
        "            #   [2]: labels\n",
        "\n",
        "            batch[2] = batch[2].to(torch.int64)\n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_input_mask = batch[1].to(device)\n",
        "            b_labels = batch[2].to(device)\n",
        "\n",
        "\n",
        "            # Always clear any previously calculated gradients before performing a\n",
        "            # backward pass. PyTorch doesn't do this automatically because\n",
        "            # accumulating the gradients is \"convenient while training RNNs\".\n",
        "            # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass (evaluate the model on this training batch).\n",
        "            # This will return the loss (rather than the model output) because we\n",
        "            # have provided the `labels`.\n",
        "            # The documentation for this `model` function is here:\n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids,\n",
        "                        token_type_ids=None,\n",
        "                        attention_mask=b_input_mask,\n",
        "                        labels=b_labels)\n",
        "\n",
        "            # The call to `model` always returns a tuple, so we need to pull the\n",
        "            # loss value out of the tuple.\n",
        "            loss = outputs[0]\n",
        "\n",
        "            # Accumulate the training loss over all of the batches so that we can\n",
        "            # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "            # single value; the `.item()` function just returns the Python value\n",
        "            # from the tensor.\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate the gradients.\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0.\n",
        "            # This is to help prevent the \"exploding gradients\" problem.\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and take a step using the computed gradient.\n",
        "            # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "            # modified based on their gradients, the learning rate, etc.\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update the learning rate.\n",
        "            scheduler.step()\n",
        "\n",
        "        # Calculate the average loss over the training data.\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        # Store the loss value for plotting the learning curve.\n",
        "        loss_values.append(avg_train_loss)\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "        print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "        # ========================================\n",
        "        #               Validation\n",
        "        # ========================================\n",
        "        # After the completion of each training epoch, measure our performance on\n",
        "        # our validation set.\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"Running Validation...\")\n",
        "\n",
        "        t0 = time.time()\n",
        "\n",
        "        # Put the model in evaluation mode--the dropout layers behave differently\n",
        "        # during evaluation.\n",
        "        model.eval()\n",
        "\n",
        "        # Tracking variables\n",
        "        eval_loss, eval_accuracy = 0, 0\n",
        "        nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "        # Evaluate data for one epoch\n",
        "        for batch in validation_dataloader:\n",
        "\n",
        "            # Add batch to GPU\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            # Unpack the inputs from our dataloader\n",
        "            b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "            # Telling the model not to compute or store gradients, saving memory and\n",
        "            # speeding up validation\n",
        "            with torch.no_grad():\n",
        "\n",
        "                # Forward pass, calculate logit predictions.\n",
        "                # This will return the logits rather than the loss because we have\n",
        "                # not provided labels.\n",
        "                # token_type_ids is the same as the \"segment ids\", which\n",
        "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "                # The documentation for this `model` function is here:\n",
        "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "                outputs = model(b_input_ids,\n",
        "                                token_type_ids=None,\n",
        "                                attention_mask=b_input_mask)\n",
        "\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            logits = outputs[0]\n",
        "\n",
        "            # Move logits and labels to CPU\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "            # Calculate the accuracy for this batch of test sentences.\n",
        "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "\n",
        "            # Accumulate the total accuracy.\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "            # Track the number of batches\n",
        "            nb_eval_steps += 1\n",
        "\n",
        "        # Report the final accuracy for this validation run.\n",
        "        print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "        print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Training complete!\")\n",
        "    return model, tokenizer, BATCH_SIZE, MAX_LEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6rtICkSVzAV"
      },
      "outputs": [],
      "source": [
        "def test_model(model, tokenizer, sentences, labels, fold_num, batch_size, max_len):\n",
        "\n",
        "    ### USE CUDA AND GPU.\n",
        "    ####################\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "        print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "    else:\n",
        "        print('No GPU available, using the CPU instead.')\n",
        "        device = torch.device(\"cpu\")\n",
        "    model.to(device)\n",
        "    ####################\n",
        "\n",
        "    ### PREPARE DATALOADERS\n",
        "    ####################\n",
        "    input_ids, attention_masks = prepare_input_ids_and_attention_masks(tokenizer, sentences, max_len)\n",
        "\n",
        "    prediction_dataloader = prepare_data_loaders(input_ids, attention_masks, labels, batch_size)\n",
        "    #########################\n",
        "\n",
        "\n",
        "    # Prediction on test set\n",
        "    print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
        "\n",
        "    # Put model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    predictions , true_labels = [], []\n",
        "    start = time.time()\n",
        "    # Predict\n",
        "    for batch in prediction_dataloader:\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up prediction\n",
        "        with torch.no_grad():\n",
        "            # Forward pass, calculate logit predictions\n",
        "            outputs = model(b_input_ids, token_type_ids=None,\n",
        "                            attention_mask=b_input_mask)\n",
        "\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Store predictions and true labels\n",
        "        predictions.append(logits)\n",
        "        true_labels.append(label_ids)\n",
        "\n",
        "    end = time.time()\n",
        "    print(\"prediction_execution_time =\", end - start)\n",
        "    print('    DONE.')\n",
        "\n",
        "    flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "    flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "\n",
        "    cm = confusion_matrix(flat_true_labels, flat_predictions, labels=[0,1])\n",
        "\n",
        "    f1_weighted = f1_score(flat_true_labels, flat_predictions, average='weighted')\n",
        "    f1_macro = f1_score(flat_true_labels, flat_predictions, average='macro')\n",
        "    f1_micro = f1_score(flat_true_labels, flat_predictions, average='micro')\n",
        "\n",
        "\n",
        "\n",
        "    print(f'f1: {f1_score(flat_true_labels, flat_predictions)}')\n",
        "    print(f'Precision: {precision_score(flat_true_labels, flat_predictions)}')\n",
        "    print(f'Recall: {recall_score(flat_true_labels, flat_predictions)}')\n",
        "    print(f'f1_macro: {f1_macro}')\n",
        "    print(f'macro-Recall: {recall_score(flat_true_labels, flat_predictions, average=\"macro\")}')\n",
        "    print(f'macro-Precision: {precision_score(flat_true_labels, flat_predictions, average=\"macro\")}')\n",
        "    print(f'f1_micro: {f1_micro}')\n",
        "    print(f'micro-Recall: {recall_score(flat_true_labels, flat_predictions, average=\"micro\")}')\n",
        "    print(f'micro-Precision: {precision_score(flat_true_labels, flat_predictions, average=\"micro\")}')\n",
        "    print(f'TN: {cm[0][0]}')\n",
        "    print(f'FN: {cm[1][0]}')\n",
        "    print(f'TP: {cm[1][1]}')\n",
        "    print(f'FP: {cm[0][1]}')\n",
        "    print(f'predictions: {flat_predictions.tolist()}')\n",
        "\n",
        "    return flat_predictions,precision_score(flat_true_labels, flat_predictions), recall_score(flat_true_labels, flat_predictions), f1_score(flat_true_labels, flat_predictions), precision_score(flat_true_labels, flat_predictions, average=\"macro\"), recall_score(flat_true_labels, flat_predictions, average=\"macro\"), f1_score(flat_true_labels, flat_predictions, average=\"macro\"), precision_score(flat_true_labels, flat_predictions, average=\"micro\"), recall_score(flat_true_labels, flat_predictions, average=\"micro\"), f1_score(flat_true_labels, flat_predictions, average=\"micro\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPqxJDrcVzby"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(corpus_name):\n",
        "    if corpus_name == 'MERGED':\n",
        "        TRAINING_DATA_URL = 'https://raw.githubusercontent.com/hurlab/GPT-PPI/main/Datasets/csv_output/merged-train.csv'\n",
        "        TEST_DATA_URL = 'https://raw.githubusercontent.com/hurlab/GPT-PPI/main/Datasets/csv_output/merged-test.csv'\n",
        "    elif corpus_name == 'AIMED':\n",
        "        TRAINING_DATA_URL = 'https://raw.githubusercontent.com/hurlab/GPT-PPI/main/Datasets/csv_output/AIMed-train.csv'\n",
        "        TEST_DATA_URL = 'https://raw.githubusercontent.com/hurlab/GPT-PPI/main/Datasets/csv_output/AIMed-test.csv'\n",
        "    elif corpus_name == 'BIOINFER':\n",
        "        TRAINING_DATA_URL = 'https://raw.githubusercontent.com/hurlab/GPT-PPI/main/Datasets/csv_output/BioInfer-train.csv'\n",
        "        TEST_DATA_URL = 'https://raw.githubusercontent.com/hurlab/GPT-PPI/main/Datasets/csv_output/BioInfer-test.csv'\n",
        "    elif corpus_name == 'HPRD50':\n",
        "        TRAINING_DATA_URL = 'https://raw.githubusercontent.com/hurlab/GPT-PPI/main/Datasets/csv_output/HPRD50-train.csv'\n",
        "        TEST_DATA_URL = 'https://raw.githubusercontent.com/hurlab/GPT-PPI/main/Datasets/csv_output/HPRD50-test.csv'\n",
        "    elif corpus_name == 'IEPA':\n",
        "        TRAINING_DATA_URL = 'https://raw.githubusercontent.com/hurlab/GPT-PPI/main/Datasets/csv_output/IEPA-train.csv'\n",
        "        TEST_DATA_URL = 'https://raw.githubusercontent.com/hurlab/GPT-PPI/main/Datasets/csv_output/IEPA-test.csv'\n",
        "    elif corpus_name == 'LLL':\n",
        "        TRAINING_DATA_URL = 'https://raw.githubusercontent.com/hurlab/GPT-PPI/main/Datasets/csv_output/LLL-train.csv'\n",
        "        TEST_DATA_URL = 'https://raw.githubusercontent.com/hurlab/GPT-PPI/main/Datasets/csv_output/LLL-test.csv'\n",
        "    else:\n",
        "        print(\"THERE IS A PROBLEM WITH THE INPUT CORPUS NAME\")\n",
        "        return\n",
        "\n",
        "\n",
        "    ### LOAD DATA INTO CSV.\n",
        "    ####################\n",
        "    train_df = pd.read_csv(TRAINING_DATA_URL)\n",
        "    test_df = pd.read_csv(TEST_DATA_URL)\n",
        "    df = pd.concat([train_df, test_df], axis=0)\n",
        "    preprocessed_sentences.append(preprocessed_sentence)\n",
        "\n",
        "    df['PreProcessedSent'] = sentence\n",
        "\n",
        "\n",
        "    sentences = df.PreProcessedSent.values\n",
        "    labels = df.isValid.values.astype(int)\n",
        "    ####################\n",
        "\n",
        "    # Report the number of sentences.\n",
        "    print('Number of sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjZD5dn6Lald"
      },
      "outputs": [],
      "source": [
        "def apply_predefined_10_fold(corpus_name):\n",
        "    all_data = prepare_dataset(corpus_name=corpus_name)\n",
        "\n",
        "    corpus_name_in_url = corpus_name\n",
        "    if corpus_name == \"AIMED\":\n",
        "        corpus_name_in_url = \"AIMed\"\n",
        "    elif corpus_name == \"BIOINFER\":\n",
        "        corpus_name_in_url = \"BioInfer\"\n",
        "\n",
        "    # binary\n",
        "    precision_scores = []\n",
        "    recall_scores = []\n",
        "    f1_scores = []\n",
        "\n",
        "    # macro\n",
        "    precision_scores_macro = []\n",
        "    recall_scores_macro = []\n",
        "    f1_scores_macro = []\n",
        "\n",
        "    # micro\n",
        "    precision_scores_micro = []\n",
        "    recall_scores_micro = []\n",
        "    f1_scores_micro = []\n",
        "\n",
        "    document_ids = all_data.docid.unique()\n",
        "\n",
        "    for fold_num in range(1,11):\n",
        "        val_docids = pd.read_csv(\"https://raw.githubusercontent.com/hurlab/GPT-PPI/main/Datasets/ppi-cv-splits/\"+corpus_name_in_url +\"/\" + corpus_name_in_url +str(fold_num)+\".txt\", header=None)\n",
        "        val_docids = val_docids[0]\n",
        "\n",
        "        train_df = all_data[~all_data['docid'].isin(val_docids)]\n",
        "        val_df = all_data[all_data['docid'].isin(val_docids)]\n",
        "\n",
        "        # train the model\n",
        "        train_sentences = train_df.PreProcessedSent.values\n",
        "        train_labels = train_df.isValid.values.astype(int)\n",
        "        model, tokenizer, batch_size, max_len = train_model(sentences=train_sentences, labels=train_labels, fold_num=fold_num, annotate_ino_terms=annotate_ino_terms, add_ino_ids_to_tokenizer=add_ino_ids_to_tokenizer)\n",
        "\n",
        "        # test the model\n",
        "        test_sentences = val_df.PreProcessedSent.values\n",
        "        test_labels = val_df.isValid.values.astype(int)\n",
        "        flat_predictions, precision, recall, f1, precision_macro, recall_macro, f1_macro, precision_micro, recall_micro, f1_micro = test_model(model=model, tokenizer=tokenizer, sentences=test_sentences, labels=test_labels, fold_num=fold_num, batch_size=batch_size, max_len=max_len)\n",
        "\n",
        "        # append model score binary\n",
        "        precision_scores.append(precision)\n",
        "        recall_scores.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "        # append model score macro\n",
        "        precision_scores_macro.append(precision_macro)\n",
        "        recall_scores_macro.append(recall_macro)\n",
        "        f1_scores_macro.append(f1_macro)\n",
        "\n",
        "        # append model score micro\n",
        "        precision_scores_micro.append(precision_micro)\n",
        "        recall_scores_micro.append(recall_micro)\n",
        "        f1_scores_micro.append(f1_micro)\n",
        "\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"f1_scores\",f1_scores)\n",
        "    print(f\"Mean-f1: {sum(f1_scores) / len(f1_scores)}\")\n",
        "    return precision_scores, recall_scores, f1_scores, precision_scores_macro, recall_scores_macro, f1_scores_macro, precision_scores_micro, recall_scores_micro, f1_scores_micro\n",
        "\n",
        "precision_scores, recall_scores, f1_scores, precision_scores_macro, recall_scores_macro, f1_scores_macro, precision_scores_micro, recall_scores_micro, f1_scores_micro = apply_predefined_10_fold(corpus_name=\"LLL\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
