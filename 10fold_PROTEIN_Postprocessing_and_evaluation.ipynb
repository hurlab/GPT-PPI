{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d62c579-0ef7-49ea-9ab4-aa831ef375d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb974549-18c4-4976-ac8c-2528d9299534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_parts_into_folds_ordered_IEPA(folder_path, i, j):\n",
    "    try:\n",
    "        # Dictionary to hold the content of each fold\n",
    "        fold_contents = {}\n",
    "\n",
    "        # List all files in the folder\n",
    "        all_files = os.listdir(folder_path)\n",
    "\n",
    "        # Regular expression to match files with pattern '1_part[1-3]_foldX.txt'\n",
    "        # Format the regex pattern with the current value of j\n",
    "        pattern_str = f'{j}_part(\\\\d+)_fold(\\\\d+)\\\\.txt'\n",
    "        pattern = re.compile(pattern_str)\n",
    "        print(pattern)\n",
    "\n",
    "        for file in sorted(all_files):  # Sort files to ensure correct order\n",
    "            match = pattern.match(file)\n",
    "            if match:\n",
    "                part_number, fold_number = match.groups()\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "\n",
    "                # Initialize fold content if not present\n",
    "                if fold_number not in fold_contents:\n",
    "                    fold_contents[fold_number] = []\n",
    "\n",
    "                # Read file content\n",
    "                with open(file_path, 'r') as f:\n",
    "                    content = f.readlines()\n",
    "\n",
    "                # Special handling for the beginning or end of each part\n",
    "                if part_number in ['1', '2'] and content and content[-1].strip() == \"Done\":\n",
    "                    content.pop()\n",
    "                if part_number in ['2', '3'] and content and \"Sentence ID\" in content[0]:\n",
    "                    content.pop(0)\n",
    "\n",
    "                # Add content to the fold\n",
    "                fold_contents[fold_number].extend(content)\n",
    "\n",
    "        # Write each fold's content to a new file\n",
    "        for fold_number, content in fold_contents.items():\n",
    "            merged_file_path = os.path.join(folder_path, f\"{j}_fold{fold_number}.txt\")\n",
    "            print(merged_file_path)\n",
    "            with open(merged_file_path, 'w') as merged_file:\n",
    "                # Prepend header if missing\n",
    "                if not content[0].startswith(\"Sentence ID\"):\n",
    "                    content.insert(0, \"Sentence ID,PPI\\n\")\n",
    "                merged_file.writelines(content)\n",
    "\n",
    "        return \"Files merged successfully.\"\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "\n",
    "#for i in range(1, 17):\n",
    "i = 15\n",
    "for j in range(1, 11):\n",
    "        # Placeholder path for the folder containing the files\n",
    "        folder_placeholder = f\"Output/gpt-4-0613_PROTEIN_Prompts_0.0/Prompt{i}/IEPA/IEPA_T0.0_Prompt{i}_Run{j}/\"\n",
    "        print(folder_placeholder)\n",
    "        merge_parts_into_folds_ordered_IEPA(folder_placeholder, i, j)\n",
    "        folder_placeholder = f\"Output/gpt-3.5-turbo-0613_PROTEIN_Prompts_0.0/Prompt{i}/IEPA/IEPA_T0.0_Prompt{i}_Run{j}/\"\n",
    "        print(folder_placeholder)\n",
    "        merge_parts_into_folds_ordered_IEPA(folder_placeholder, i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd6bc0f-a3d4-42d2-89ca-4a9c3175651c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_parts_into_folds_ordered(folder_path,i,j):\n",
    "    try:\n",
    "        # Dictionary to hold the content of each fold\n",
    "        fold_contents = {}\n",
    "\n",
    "        # List all files in the folder\n",
    "        all_files = os.listdir(folder_path)\n",
    "\n",
    "        # Regular expression to match files with pattern '1_part[1-2]_foldX.txt'\n",
    "        # Format the regex pattern with the current value of j\n",
    "        pattern_str = f'{j}_part(\\\\d+)_fold(\\\\d+)\\\\.txt'\n",
    "        pattern = re.compile(pattern_str)\n",
    "        print(pattern)\n",
    "\n",
    "        for file in sorted(all_files):  # Sort files to ensure correct order\n",
    "            match = pattern.match(file)\n",
    "            if match:\n",
    "                part_number, fold_number = match.groups()\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "\n",
    "                # Initialize fold content if not present\n",
    "                if fold_number not in fold_contents:\n",
    "                    fold_contents[fold_number] = []\n",
    "\n",
    "                # Read file content\n",
    "                with open(file_path, 'r') as f:\n",
    "                    content = f.readlines()\n",
    "\n",
    "                # Conditionally remove \"Done\" line from the end of part1\n",
    "                if part_number == '1' and content and content[-1].strip() == \"Done\":\n",
    "                    content.pop()\n",
    "\n",
    "                # Conditionally remove \"Sentence ID\" line from the beginning of part2\n",
    "                if part_number == '2' and content and \"Sentence ID\" in content[0]:\n",
    "                    content.pop(0)\n",
    "\n",
    "                # Add content to the fold\n",
    "                fold_contents[fold_number].extend(content)\n",
    "\n",
    "        # Write each fold's content to a new file\n",
    "        for fold_number, content in fold_contents.items():\n",
    "            merged_file_path = os.path.join(folder_path, f\"{j}_fold{fold_number}.txt\")\n",
    "            print(merged_file_path)\n",
    "            #with open(merged_file_path, 'w') as merged_file:\n",
    "            #    merged_file.writelines(content)\n",
    "            \n",
    "            with open(merged_file_path, 'w') as merged_file:\n",
    "                # Prepend header if missing\n",
    "                if not content[0].startswith(\"Sentence ID\"):\n",
    "                    content.insert(0, \"Sentence ID,PPI\\n\")\n",
    "                merged_file.writelines(content)\n",
    "\n",
    "        return \"Files merged successfully.\"\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "for i in range(1,17):\n",
    "    for j in range(1, 11):\n",
    "        folder_placeholder = f\"Output/gpt-4-0613_PROTEIN_Prompts_0.0/Prompt{i}/LLL/LLL_T0.0_Prompt{i}_Run{j}/\"\n",
    "        print(folder_placeholder)\n",
    "        merge_parts_into_folds_ordered(folder_placeholder, i, j)\n",
    "        folder_placeholder = f\"Output/gpt-3.5-turbo-0613_PROTEIN_Prompts_0.0/Prompt{i}/LLL/LLL_T0.0_Prompt{i}_Run{j}/\"\n",
    "        print(folder_placeholder)\n",
    "        merge_parts_into_folds_ordered(folder_placeholder, i, j)\n",
    "        folder_placeholder = f\"Output/gpt-4-0613_PROTEIN_Prompts_0.0/Prompt{i}/HPRD50/HPRD50_T0.0_Prompt{i}_Run{j}/\"\n",
    "        print(folder_placeholder)\n",
    "        merge_parts_into_folds_ordered(folder_placeholder, i, j)\n",
    "        folder_placeholder = f\"Output/gpt-3.5-turbo-0613_PROTEIN_Prompts_0.0/Prompt{i}/HPRD50/HPRD50_T0.0_Prompt{i}_Run{j}/\"\n",
    "        print(folder_placeholder)\n",
    "        merge_parts_into_folds_ordered(folder_placeholder, i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1a34a6-eaf6-4144-9b95-a9e745975929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_header_to_txt_files_in_structure(base_directory):\n",
    "    try:\n",
    "        # Define patterns for the directory structure\n",
    "        patterns = [\n",
    "            \"gpt-4-0613_PROTEIN_Prompts_0.0\",\n",
    "            \"gpt-3.5-turbo-0613_PROTEIN_Prompts_0.0\"\n",
    "        ]\n",
    "\n",
    "        for i in range(1, 17):\n",
    "            for j in range(1, 11):\n",
    "                for pattern in patterns:\n",
    "                    # Construct directory paths\n",
    "                    dir_paths = [\n",
    "                        os.path.join(base_directory, pattern, f\"Prompt{i}\", \"LLL\", f\"LLL_T0.0_Prompt{i}_Run{j}\"),\n",
    "                        os.path.join(base_directory, pattern, f\"Prompt{i}\", \"HPRD50\", f\"HPRD50_T0.0_Prompt{i}_Run{j}\"),\n",
    "                        os.path.join(base_directory, pattern, f\"Prompt{i}\", \"IEPA\", f\"IEPA_T0.0_Prompt{i}_Run{j}\")\n",
    "                    ]\n",
    "\n",
    "                    for dir_path in dir_paths:\n",
    "                        if os.path.exists(dir_path):\n",
    "                            # Process each .txt file in the directory\n",
    "                            for file in os.listdir(dir_path):\n",
    "                                if file.endswith('.txt'):\n",
    "                                    file_path = os.path.join(dir_path, file)\n",
    "                                    with open(file_path, 'r') as f:\n",
    "                                        content = f.readlines()\n",
    "\n",
    "                                    # Check if the first line is the expected header\n",
    "                                    if not content or not content[0].strip().startswith(\"Sentence ID,PPI\"):\n",
    "                                        content.insert(0, \"Sentence ID,PPI\\n\")\n",
    "                                        with open(file_path, 'w') as f:\n",
    "                                            f.writelines(content)\n",
    "\n",
    "        return \"Header added successfully where necessary.\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "# Example usage\n",
    "base_directory = 'Output/'  # Ensure this is correctly indented\n",
    "add_header_to_txt_files_in_structure(base_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53763f8a-f18f-493c-9ae1-cb675bff36ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_PROTEIN(data_path, dataset, output_path, run):\n",
    "    run_standard = ['fold1.txt', 'fold2.txt', 'fold3.txt','fold4.txt', 'fold5.txt', 'fold6.txt', 'fold7.txt', 'fold8.txt', 'fold9.txt','fold10.txt']\n",
    "    data_path_standard = f'Datasets/PROTEIN_DATA/{dataset}/PROTEIN_all/standard/'\n",
    "    for f in range(10):\n",
    "        fold = f+1      \n",
    "        df_predicted_processed = pd.read_csv(os.path.join(data_path, run[f]), sep = ',', on_bad_lines= 'skip')     \n",
    "        df_predicted_processed = df_predicted_processed.dropna()\n",
    "        df_predicted_processed = df_predicted_processed.drop(df_predicted_processed[df_predicted_processed['Sentence ID'] == 'Done'].index)\n",
    "        df_predicted_processed.columns = ['Sentence ID','PPI']\n",
    "        interaction_processed = pd.read_csv(os.path.join(data_path_standard, run_standard[f]), sep = '\\t')\n",
    "        y = interaction_processed['isValid']\n",
    "     \n",
    "        y_pred = df_predicted_processed ['PPI'].astype(bool)\n",
    "\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        precision = precision_score(y, y_pred)\n",
    "        recall = recall_score(y, y_pred)\n",
    "        f1 = f1_score(y, y_pred)\n",
    "        \n",
    "        print(\"Fold = \",fold)\n",
    "        print(f\"Precision: {precision}\")\n",
    "        print(f\"Recall: {recall}\")\n",
    "        print(f\"F1 Score: {f1}\")\n",
    "        \n",
    "        print(data_path)\n",
    "        \n",
    "        with open(output_path, \"a\") as f:\n",
    "          print (data_path, fold,',', \"{:.4f}\".format(precision), ',', \"{:.4f}\".format(recall), ',',\"{:.4f}\".format(f1), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2954ac88-0ab4-46a2-ab49-2d4774ebfe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_no = \"PROTEIN_Prompts\"\n",
    "def output_path(Run_no, dataset, temperature, prompt_no, model_engine):\n",
    "    base = \"Output/\" + model_engine + \"_\"+section_no+\"_\" + str(temperature) + \"/\" + str(prompt_no) +\"/\"+dataset + \"/\"\n",
    "    extension_path = dataset + \"_T\" + str(temperature) + \"_\" + prompt_no + \"_Run\" + str(Run_no)+'/'\n",
    "\n",
    "    # Join the base path and extension path\n",
    "    Implementation_base_path_output = os.path.join(base, extension_path)\n",
    "    os.makedirs(Implementation_base_path_output, exist_ok=True)\n",
    "    return Implementation_base_path_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5a80c9-c20c-4a9e-a187-7f4173671f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for t in range(14):\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "temperature = 0.0\n",
    "\n",
    "prompt_no =15\n",
    "\n",
    "model_engines = [\"gpt-3.5-turbo-0613\", \"gpt-4-0613\"]\n",
    "datasets = [\"LLL\", \"HPRD50\", \"IEPA\"]\n",
    "\n",
    "\n",
    "for model_engine in model_engines:\n",
    "    for dataset in datasets:\n",
    "        for R in range(1,11):\n",
    "              query_no = f\"Prompt{prompt_no}\"\n",
    "              run = [f'{R}_fold{i}.txt' for i in range(1, 11)]\n",
    "              output_path_R = f'Output/{model_engine}_PROTEIN_Prompts_0.0/{query_no}/{dataset}/Evaluation_{dataset}_T{temperature}_{query_no}_Run{R}.csv'\n",
    "              Implementation_base_path_output_Run = output_path(R, dataset, temperature, query_no, model_engine)\n",
    "              evaluation_PROTEIN(Implementation_base_path_output_Run, dataset, output_path_R , run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea5a981-3a21-4cba-b33e-48be5fad0e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "model_engines = [\"gpt-3.5-turbo-0613\", \"gpt-4-0613\"]\n",
    "datasets = [\"LLL\", \"HPRD50\", \"IEPA\"]\n",
    "\n",
    "\n",
    "for model_engine in model_engines:\n",
    "    for dataset in datasets:\n",
    "\n",
    "        for prompt_no in range (15, 16):\n",
    "            prompt_no = 15\n",
    "            # define the folder path containing the CSV files\n",
    "            folder_path = f\"Output/{model_engine}_PROTEIN_Prompts_0.0/Prompt{prompt_no}/{dataset}/\"\n",
    "\n",
    "            # create an empty dataframe to store the results\n",
    "            results_df = pd.DataFrame(columns=[\"filename\", \"average precision\", \"average recall\", \"average f1 score\"])\n",
    "\n",
    "            # loop through the CSV files in the folder\n",
    "            for filename in os.listdir(folder_path):\n",
    "                if filename.endswith(\".csv\"):\n",
    "                    # read the CSV file into a dataframe\n",
    "                    df = pd.read_csv(os.path.join(folder_path, filename), header=None, names=[\"filename\", \"precision\", \"recall\", \"f1 score\"])\n",
    "\n",
    "                    # calculate the average precision, recall, and f1 score for each file\n",
    "                    avg_precision = df[\"precision\"].mean()\n",
    "                    avg_recall = df[\"recall\"].mean()\n",
    "                    avg_f1_score = df[\"f1 score\"].mean()\n",
    "\n",
    "                    # add the results to the results dataframe\n",
    "                    results_df = results_df.append({\n",
    "                        \"filename\": filename,\n",
    "                        \"average precision\": avg_precision,\n",
    "                        \"average recall\": avg_recall,\n",
    "                        \"average f1 score\": avg_f1_score\n",
    "                    }, ignore_index=True)\n",
    "\n",
    "            # sort the dataframe based on the \"T0.0\" column\n",
    "            results_df = results_df.sort_values(by=\"filename\")\n",
    "\n",
    "            # extract the \"T0.0\"-like part of the filenames and store them in a new column called \"T_value\"\n",
    "            results_df[\"T_value\"] = results_df[\"filename\"].str.extract(\"T(\\d+\\.\\d+)\")\n",
    "\n",
    "            # create a new dataframe with only the shortened filename and the average precision, recall, and f1 score columns\n",
    "            shortened_df = results_df[[\"T_value\", \"average precision\", \"average recall\", \"average f1 score\"]]\n",
    "\n",
    "            # sort the shortened dataframe by the \"T_value\" column\n",
    "            shortened_df = shortened_df.sort_values(by=\"T_value\")\n",
    "\n",
    "            # Convert average precision, recall, and F1 score to percentages\n",
    "            shortened_df[['average precision', 'average recall', 'average f1 score']] *= 100\n",
    "            shortened_df[['average precision', 'average recall', 'average f1 score']] = shortened_df[['average precision', 'average recall', 'average f1 score']].round(2)\n",
    "\n",
    "            # Load precision, recall, and F1 scores from the DataFrame\n",
    "            precision_scores = shortened_df['average precision']\n",
    "            recall_scores = shortened_df['average recall']\n",
    "            f1_scores = shortened_df['average f1 score']\n",
    "\n",
    "            # Calculate the average scores\n",
    "            avg_precision = round(precision_scores.mean(), 2)\n",
    "            avg_recall = round(recall_scores.mean(), 2)\n",
    "            avg_f1 = round(f1_scores.mean(), 2)\n",
    "            t = 0.0\n",
    "            # Print the average scores\n",
    "            print(\"Average Precision:\", avg_precision)\n",
    "            print(\"Average Recall:\", avg_recall)\n",
    "            print(\"Average F1 Score:\", avg_f1)\n",
    "\n",
    "            # Create a new row with the average scores\n",
    "            new_row = {'T_value': t,'average precision': avg_precision, 'average recall': avg_recall, 'average f1 score': avg_f1}\n",
    "\n",
    "            # Append the new row to the DataFrame\n",
    "            shortened_df = shortened_df.append(new_row, ignore_index=True)\n",
    "\n",
    "            filepath = os.path.join(folder_path, 'Average_Score.csv')\n",
    "            shortened_df.to_csv(filepath, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openaienv",
   "language": "python",
   "name": "openaienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
