{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdda649-ee9f-4747-865a-094e04280418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import io\n",
    "import openai\n",
    "from transformers.models.imagegpt.modeling_imagegpt import IMAGEGPT_INPUTS_DOCSTRING\n",
    "from transformers import GPT2Tokenizer\n",
    "import pandas as pd \n",
    "from pandas.io import json\n",
    "from numpy import nan\n",
    "import time\n",
    "import csv\n",
    "import shutil\n",
    "import datetime\n",
    "import pytz\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bc610a-3988-419c-b3b6-82f22c9a8d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee02750-e220-4b10-936d-11afae3b340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"OPENAI_API_KEY\"\n",
    "section_no = \"N_fold_PROTEIN_Prompts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3c2c0d-17dc-4f52-bef6-6679a0839976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate All the variations \n",
    "import itertools\n",
    "import random\n",
    "\n",
    "def get_combinations(prompt_paths, model_engines, datasets, folds_LLL, folds_HPRD50, folds_IEPA):\n",
    "    combinations = []\n",
    "    dataset_folds = {\n",
    "        'LLL': folds_LLL,\n",
    "        'HPRD50': folds_HPRD50,\n",
    "        'IEPA': folds_IEPA\n",
    "    }\n",
    "    for dataset in datasets:\n",
    "        current_prompts = prompt_paths\n",
    "        current_folds = dataset_folds[dataset]\n",
    "        for combination in itertools.product(current_prompts, model_engines, [dataset], current_folds):\n",
    "            combinations.append(combination)\n",
    "    random.shuffle(combinations)\n",
    "    return combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65a4769-d80f-4c8a-9ff2-91117e63d7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_input(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            query = file.read()\n",
    "            file_name = file_path.split('/')[-1]  # Extract the file name from the path\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{file_path} not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466ba554-20ea-4c02-90f2-56fbf2a42cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_path(Run_no, dataset, temperature, prompt_no, model_engine):\n",
    "    base = \"Output/\" + model_engine + \"_\"+section_no+\"_\" + str(temperature) + \"/\" + str(prompt_no) +\"/\"+dataset + \"/\"\n",
    "    extension_path = dataset + \"_T\" + str(temperature) + \"_\" + prompt_no + \"_Run\" + str(Run_no)+'/'\n",
    "    Implementation_base_path_output = os.path.join(base, extension_path)\n",
    "    os.makedirs(Implementation_base_path_output, exist_ok=True)\n",
    "    return Implementation_base_path_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcea27d1-4892-4a2f-b317-5a06d407ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_input_tokens(folder_path):\n",
    "  tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "  file_names = os.listdir(folder_path)\n",
    "  max_num_input_lines = 0\n",
    "\n",
    "  # Loop through each file and calculate number of tokens\n",
    "  for file_name in file_names:\n",
    "      # Check if file is a text file\n",
    "      if file_name.endswith(\".txt\"):\n",
    "          # Read file contents\n",
    "          with open(os.path.join(folder_path, file_name), \"r\" , encoding='utf-8') as f:\n",
    "              file_contents = f.read()\n",
    "          with open(os.path.join(folder_path, file_name), \"r\" , encoding='utf-8') as fp:\n",
    "              num_input_line = len(fp.readlines())\n",
    "              print(\"num_input_line:\",num_input_line)\n",
    "          # Calculate number of tokens\n",
    "          num_tokens = len(tokenizer.encode(file_contents))\n",
    "          if (num_input_line>max_num_input_lines):\n",
    "            max_num_input_lines = num_input_line\n",
    "          print(f\"{file_name}: {num_tokens} tokens\")\n",
    "\n",
    "  total_max_tokens = max_num_input_lines*10 + 30\n",
    "\n",
    "  # Loop through each file and calculate number of tokens\n",
    "  for file_name in file_names:\n",
    "      # Check if file is a text file\n",
    "      if file_name.endswith(\".txt\"):\n",
    "          # Read file contents\n",
    "          with open(os.path.join(folder_path, file_name), \"r\" , encoding='utf-8') as f:\n",
    "              file_contents = f.read()\n",
    "          # Calculate number of tokens\n",
    "          num_tokens = len(tokenizer.encode(file_contents))\n",
    "          total = num_tokens+ total_max_tokens\n",
    "          print(f\"{file_name}: Input: {num_tokens} tokens, Output: {total_max_tokens} : Total: {total}\")\n",
    "  print(total_max_tokens)\n",
    "  return total_max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee242ae-2704-441f-bee2-2eacbd9ea550",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ChatGPT Completion\n",
    "def get_completion(BACKOFF_OCCURRED_, model, query, Sentences, max_tokens,  temperature):\n",
    "    prompt = f\"\"\"\n",
    "        {query}\n",
    "        {Sentences}\n",
    "        \"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    tries = 0\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "            )\n",
    "            call_time = time.time()\n",
    "            time_f = call_time - start_time\n",
    "            break\n",
    "        except (openai.error.RateLimitError, openai.error.ServiceUnavailableError, openai.error.APIError) as e:\n",
    "            tries += 1\n",
    "            max_backoff = 60 \n",
    "            backoff_time =  min(5 + 5*(tries ** 2), max_backoff)\n",
    "            with BACKOFF_OCCURRED_.get_lock():\n",
    "                BACKOFF_OCCURRED_.value = True\n",
    "            time.sleep(backoff_time)\n",
    "            with BACKOFF_OCCURRED_.get_lock():\n",
    "                BACKOFF_OCCURRED_.value = False\n",
    "            print(\"Backoff Released\\n\")\n",
    "    \n",
    "    message = response['choices'][0]['message']['content']\n",
    "    output_token = response['usage']['completion_tokens']\n",
    "    input_token = response['usage']['prompt_tokens']\n",
    "    \n",
    "    now_utc = datetime.datetime.now(pytz.utc)\n",
    "    timezone = pytz.timezone(\"US/Central\")\n",
    "    now_eastern = now_utc.astimezone(timezone)\n",
    "    time_stamp = str(now_eastern)\n",
    "    return message, input_token, output_token, time_f, time_stamp, tries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081c0294-6d97-418a-b60e-d523c5d81daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "def call_ChatGPT(BACKOFF_OCCURRED_, current_folds, Implementation_base_path_input, Implementation_base_path_output, Run, total_max_tokens, temperature, query, dataset, model):\n",
    "        RETRY_COUNT = 5\n",
    "        input_file_path = os.path.join(Implementation_base_path_input, f'{current_folds}')\n",
    "        output_file_path = os.path.join(Implementation_base_path_output, f'{Run}_{current_folds}')\n",
    "        time_track_path = os.path.join(Implementation_base_path_output, f'{temperature}_time_track.csv')\n",
    "\n",
    "        while BACKOFF_OCCURRED_.value:\n",
    "            print(\"\\nBackoff occurred! Pausing all threads for a set duration...\")\n",
    "            sleep_time = random.randint(1, 5)\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "        for attempt in range(RETRY_COUNT):\n",
    "            try:\n",
    "                with open(input_file_path) as f:\n",
    "                    Sentences = f.read()\n",
    "                message, input_token, output_token, time_f, time_stamp, tries = get_completion(BACKOFF_OCCURRED_, model, query, Sentences=Sentences, max_tokens=total_max_tokens, temperature=temperature)\n",
    "\n",
    "                last_line = message.strip().split('\\n')[-1]\n",
    "                status = \"Complete\" if \"Done\" in last_line else \"Possibly Incomplete\"\n",
    "\n",
    "                print(f\"Fold ={current_folds}, Run= {Run}, Temperature={temperature}, {status}. output_file_path: {output_file_path}\")\n",
    "\n",
    "                with open(time_track_path, \"a\") as f:\n",
    "                    print(dataset, ',', Run, ',', current_folds, ',', temperature, ',', input_token, ',', output_token, ',', time_f, ',', time_stamp, ',', tries, file=f)\n",
    "\n",
    "                with open(output_file_path, \"w\") as f:\n",
    "                    print(message, file=f)\n",
    "                    \n",
    "                break\n",
    "                \n",
    "            except Exception as e:  # Catch general exceptions. Be specific if you know which exceptions to expect\n",
    "                    print(f\"Error occurred: {e}. Retrying {attempt+1}/{RETRY_COUNT}. output_file_path: {output_file_path}\")\n",
    "                    time.sleep(1)  \n",
    "\n",
    "            else:  # This block will be executed if the for loop completed without 'break', i.e., if all attempts failed.\n",
    "                print(f\"All {RETRY_COUNT} retries failed for fold {current_folds} at Run {Run}. output_file_path: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395e4823-53df-4751-a984-168f0a8cb0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "BACKOFF_OCCURRED_ = multiprocessing.Value('b', False)  # 'b' denotes a boolean\n",
    "\n",
    "def execute_code(run_no, prompt_no, model_engine, dataset, current_folds):\n",
    "    global BACKOFF_OCCURRED_\n",
    "    print(run_no, prompt_no, model_engine, dataset, current_folds)\n",
    "    prompt_path = f\"Prompts/PROTEIN_Prompts/{prompt_no}.txt\"\n",
    "    print(prompt_path)\n",
    "    temperature = 0.0 \n",
    "    query = user_input(prompt_path)\n",
    "    Implementation_base_path_input = 'Datasets/PROTEIN_DATA/' + dataset + '/N_fold/N_fold_txt/'\n",
    "    print(Implementation_base_path_input)\n",
    "    Implementation_base_path_output = output_path(run_no, dataset, temperature, prompt_no, model_engine)\n",
    "    total_max_tokens = count_input_tokens(Implementation_base_path_input)\n",
    "    call_ChatGPT(BACKOFF_OCCURRED_, current_folds, Implementation_base_path_input, Implementation_base_path_output, run_no, total_max_tokens, temperature, query, dataset,model = model_engine)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    prompt_no = ['Prompt15']\n",
    "    model_engines = [\"gpt-3.5-turbo-0613\", \"gpt-4-0613\"]\n",
    "    datasets = [\"LLL\", \"HPRD50\", \"IEPA\"]\n",
    "\n",
    "    folds_LLL = [f\"fold{i}.txt\" for i in range(1, 37)]\n",
    "\n",
    "    # HPRD50 - Fold 4 has part1 and part2\n",
    "    folds_HPRD50 = [f\"fold{i}.txt\" for i in range(1, 22)]\n",
    "    folds_HPRD50.remove(\"fold4.txt\")\n",
    "    folds_HPRD50.extend([\"part1_fold4.txt\", \"part2_fold4.txt\"])\n",
    "\n",
    "    folds_IEPA = [f\"fold{i}.txt\" for i in range(1, 16)]\n",
    "    folds_HPRD50.remove(\"fold5.txt\", \"fold14.txt\")\n",
    "    folds_HPRD50.extend([\"part1_fold5.txt\", \"part2_fold5.txt\", \"part1_fold14.txt\", \"part2_fold14.txt\"])\n",
    "\n",
    "    for Run_no in range(1, 11):\n",
    "        # Get all combinations\n",
    "        all_combinations = get_combinations(prompt_no, model_engines, datasets, folds_LLL, folds_HPRD50, folds_IEPA)\n",
    "        # Print the total number of combinations\n",
    "        print(f\"Total number of combinations: {len(all_combinations)}\")\n",
    "        args = [(Run_no,) + combo for combo in all_combinations]\n",
    "        no_of_workers = 20\n",
    "\n",
    "        with multiprocessing.Pool(no_of_workers) as pool:\n",
    "            pool.starmap(execute_code, args)\n",
    "            time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openaienv",
   "language": "python",
   "name": "openaienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
