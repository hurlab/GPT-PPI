{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a589226d-7c6e-4368-931f-6a4d25322291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import io\n",
    "import openai\n",
    "from transformers.models.imagegpt.modeling_imagegpt import IMAGEGPT_INPUTS_DOCSTRING\n",
    "from transformers import GPT2Tokenizer\n",
    "import pandas as pd \n",
    "from pandas.io import json\n",
    "from numpy import nan\n",
    "import time\n",
    "import csv\n",
    "import shutil\n",
    "import datetime\n",
    "import pytz\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import time\n",
    "import random\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac35069-13e5-42d9-8702-a51f6ea18d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combinations(prompt_paths, model_engines, datasets, folds_LLL, folds_HPRD50, folds_IEPA):\n",
    "    combinations = []\n",
    "\n",
    "    # Create dictionary for dataset-specific folds\n",
    "    dataset_folds = {\n",
    "        'LLL': folds_LLL,\n",
    "        'HPRD50': folds_HPRD50,\n",
    "        'IEPA': folds_IEPA\n",
    "    }\n",
    "\n",
    "    # Exclude 'BASE' prompts for 'LLL'\n",
    "    prompt_paths_LLL = [path for path in prompt_paths if \"BASE\" not in path]\n",
    "\n",
    "    # Iterate over each dataset to get the dataset-specific folds and prompts\n",
    "    for dataset in datasets:\n",
    "        current_prompts = prompt_paths_LLL if dataset == \"LLL\" else prompt_paths\n",
    "        current_folds = dataset_folds[dataset]\n",
    "        \n",
    "        # Use itertools.product to create combinations for the current dataset\n",
    "        for combination in itertools.product(current_prompts, model_engines, [dataset], current_folds):\n",
    "            combinations.append(combination)\n",
    "    # Shuffle the list of combinations\n",
    "    random.shuffle(combinations)\n",
    "    \n",
    "    return combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b095ebb5-3311-49c4-ba11-6b031f7cc9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_path(Run_no, dataset, temperature, prompt_type, model_engine):\n",
    "    base = \"Output/\" + model_engine + \"_Final_Prompts_\" + str(temperature) + \"/Prompt\" + str(prompt_type) +\"/\"+dataset + \"/\"\n",
    "    extension_path = dataset + \"_T\" + str(temperature) + \"_\" + prompt_type + \"_Run\" + str(Run_no)+'/'\n",
    "    Implementation_base_path_output = os.path.join(base, extension_path)\n",
    "    os.makedirs(Implementation_base_path_output, exist_ok=True)\n",
    "    \n",
    "    return Implementation_base_path_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d16e720-2642-473b-a137-46a7674d4cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_Entity(str):\n",
    "  str = str.replace('\\t', '')\n",
    "  str = str.replace('\"', '')\n",
    "  str = str.replace('(', '')\n",
    "\n",
    "  str = str.replace(')', '')\n",
    "  str = str.replace(' ', '')\n",
    "  str = re.sub(r'\\s', '', str)\n",
    "  str = str.replace('~', '')\n",
    "  str = str.replace('-', '')\n",
    "  str = str.replace(',', '')\n",
    "  str = str.lower()\n",
    "  str = str.replace('operon', '')\n",
    "  str = str.replace('regulon', '')\n",
    "  str = str.replace('genes', '')\n",
    "  str = str.replace('gene', '')\n",
    "  str = str.replace('mutant', '')\n",
    "  str = str.replace('phosphatase', '')\n",
    "  str = str.replace('complex', '')\n",
    "  str = str.strip()\n",
    "\n",
    "  return str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2d951e-2f94-4b85-bfe9-21ad76810bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_string_by_slash(input_string):\n",
    "    parts = input_string.split(\"/\")\n",
    "    return parts\n",
    "\n",
    "def split_string_by_comma(input_string):\n",
    "    parts = input_string.split(\",\")\n",
    "    return parts\n",
    "\n",
    "def check_word_in_predicted_entity(predicted, original):\n",
    "    pattern = r'\\b{}\\b'.format(re.escape(original))\n",
    "    return bool(re.search(pattern, predicted))\n",
    "\n",
    "def sort_entities(row):\n",
    "        return tuple(sorted([row['E1'], row['E2']]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa8e491-3a21-4202-a015-09322c66d2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(data_path, run, output_path, interaction_processed):#, Norm_Predicted_Protein_Sciminer):\n",
    "\n",
    "    #skipped_lines_total = 0\n",
    "    for f in range(10):\n",
    "        fold = f + 1\n",
    "        print(run[f])\n",
    "        print(data_path)\n",
    "        print(os.path.join(data_path, run[f]))\n",
    "\n",
    "        with open(os.path.join(data_path, run[f]), 'r') as file:\n",
    "            # Filter out blank lines and store them in a list\n",
    "            lines = [line for line in file if line.strip()]\n",
    "            # Now total_lines should be the count of the filtered lines\n",
    "            total_lines = len(lines)    \n",
    "        \n",
    "\n",
    "        # Read the first line of the CSV\n",
    "        with open(os.path.join(data_path, run[f]), 'r') as file:\n",
    "            first_line = file.readline().strip()\n",
    "\n",
    "        # Check if the first line contains \"Output:\"\n",
    "        if \"Output:\" in first_line:\n",
    "            # Read the file skipping the first row and using comma as separator\n",
    "            df_predicted = pd.read_csv(os.path.join(data_path, run[f]), sep=',', skiprows=1, on_bad_lines='skip')\n",
    "        else:\n",
    "            # Directly read the file using comma as separator\n",
    "            df_predicted = pd.read_csv(os.path.join(data_path, run[f]), sep=',', on_bad_lines='skip')\n",
    "\n",
    "        \n",
    "        # Subtracting 1 for the header row\n",
    "        skipped_lines = total_lines - df_predicted.shape[0] - 1 if total_lines > 0 else 0\n",
    "\n",
    "        print(f\"Number of lines skipped: {skipped_lines}\")\n",
    "        #skipped_lines_total = skipped_lines + skipped_lines_total\n",
    "        \n",
    "        # Check if the 'status' column of the last row contains 'DONE'\n",
    "        if 'done' in map(str.lower, map(str, df_predicted.iloc[-1].values)):\n",
    "            df_predicted = df_predicted.iloc[:-1]\n",
    "            \n",
    "        print(df_predicted)\n",
    "        \n",
    "        df_predicted = df_predicted.dropna()\n",
    "        #df_predicted.columns = ['Sentence_ID', 'E1', 'E2', 'Interaction']\n",
    "        df_predicted.columns = ['Sentence_ID', 'E1', 'E2', 'Interaction']\n",
    "        df_predicted = df_predicted[~df_predicted['Interaction'].str.contains(\"not \")]   ###########\n",
    "\n",
    "        df_predicted['Sorted_Entities'] = df_predicted.apply(sort_entities, axis=1)\n",
    "\n",
    "        df_combined_interactions = df_predicted.groupby(['Sentence_ID', 'Sorted_Entities'])['Interaction'].apply(', '.join).reset_index()\n",
    "\n",
    "        df_combined_interactions_deduplicated = df_combined_interactions.drop_duplicates(subset=['Sentence_ID', 'Sorted_Entities'], keep='first')\n",
    "        df_combined_interactions_deduplicated = df_combined_interactions_deduplicated[~df_combined_interactions_deduplicated.apply(lambda row: row.astype(str).str.contains(\"Done\").any(), axis=1)]\n",
    "        df_combined_interactions_deduplicated = df_combined_interactions_deduplicated.reset_index(drop=True)\n",
    "\n",
    "        df_combined_interactions_deduplicated[['E1', 'E2']] = pd.DataFrame(df_combined_interactions_deduplicated['Sorted_Entities'].tolist(), index=df_combined_interactions_deduplicated.index)\n",
    "        df_predicted_processed = df_combined_interactions_deduplicated.drop(columns=['Sorted_Entities'])\n",
    "        # Resetting the index of the DataFrame\n",
    "        df_predicted_processed.reset_index(drop=True, inplace=True)  ########\n",
    "        No_of_predicted_interaction_pairs = len(df_predicted_processed.index)\n",
    "        print(No_of_predicted_interaction_pairs)\n",
    "        TP = 0\n",
    "        Partial_TP = 0\n",
    "        for i in range(len(df_predicted_processed.index)):\n",
    "            match_found = False\n",
    "            partial_match_found = False\n",
    "            for j in range(len(interaction_processed.index)):\n",
    "                Predicted_Sentence_ID = df_predicted_processed.iloc[i][\"Sentence_ID\"]\n",
    "                Predicted_Sentence_ID = re.sub(r'\\s', '', Predicted_Sentence_ID)\n",
    "                \n",
    "                #Norm_Predicted_SID = Norm_Predicted_Protein_Sciminer[Norm_Predicted_Protein_Sciminer[\"Sentence_ID\"] == Predicted_Sentence_ID]\n",
    "                #Norm_Original_SID = Norm_Original_Protein_Sciminer[Norm_Original_Protein_Sciminer[\"Sentence_ID\"] == Predicted_Sentence_ID]\n",
    "\n",
    "                if interaction_processed.iloc[j][\"Fold\"] == fold and Predicted_Sentence_ID == interaction_processed.iloc[j][\"sen_id\"]:\n",
    "                    str1_raw = df_predicted_processed.iloc[i][\"E1\"]\n",
    "                    str2_raw = df_predicted_processed.iloc[i][\"E2\"]\n",
    "\n",
    "                    str3_raw = interaction_processed.iloc[j][\"E1\"]\n",
    "                    str4_raw = interaction_processed.iloc[j][\"E2\"]\n",
    "                    \n",
    "\n",
    "                    str1 = normalized_Entity(str1_raw)\n",
    "                    str2 = normalized_Entity(str2_raw)\n",
    "                    str3 = normalized_Entity(str3_raw)\n",
    "                    str4 = normalized_Entity(str4_raw)\n",
    "\n",
    "                    result1 = split_string_by_slash(str1_raw)\n",
    "                    result2 = split_string_by_slash(str2_raw)\n",
    "                    \n",
    "                    result1_comma = split_string_by_comma(str1_raw)\n",
    "                    result2_comma = split_string_by_comma(str2_raw)\n",
    "\n",
    "                    if len(result1) > 1:\n",
    "                        for part in result1:\n",
    "                            normalized_part = normalized_Entity(part)\n",
    "                            if (normalized_part == str3 and str2 == str4) or (normalized_part == str4 and str2 == str3):\n",
    "                                match_found = True\n",
    "                                break\n",
    "                                                             \n",
    "                    elif len(result2) > 1:\n",
    "                        for part in result2:\n",
    "                            normalized_part = normalized_Entity(part)\n",
    "                            if (str1 == str3 and normalized_part == str4) or (str1 == str4 and normalized_part == str3):\n",
    "                                match_found = True\n",
    "                                break\n",
    "                            \n",
    "                                \n",
    "                    if len(result1_comma) > 1:\n",
    "                        for part in result1_comma:\n",
    "                            normalized_part = normalized_Entity(part)\n",
    "                            if (normalized_part == str3 and str2 == str4) or (normalized_part == str4 and str2 == str3):\n",
    "                                print(str1, str2, str3, str4)\n",
    "                                match_found = True\n",
    "                                break\n",
    "                                                  \n",
    "                    elif len(result2_comma) > 1:\n",
    "                        for part in result2_comma:\n",
    "                            normalized_part = normalized_Entity(part)\n",
    "                            if (str1 == str3 and normalized_part == str4) or (str1 == str4 and normalized_part == str3):\n",
    "                                print(str1, str2, str3, str4)\n",
    "                                match_found = True\n",
    "                                break\n",
    "                                \n",
    "                    elif (str1 == str3 and str2 == str4) or (str1 == str4 and str2 == str3):\n",
    "                        match_found = True\n",
    "                        break\n",
    "                    elif check_word_in_predicted_entity(str1_raw.lower(), str3_raw.lower()) and check_word_in_predicted_entity(str2_raw.lower(), str4_raw.lower()):\n",
    "                        partial_match_found = True\n",
    "                        break\n",
    "                    elif check_word_in_predicted_entity(str1_raw.lower(), str3_raw.lower()) and str2 == str4:\n",
    "                        partial_match_found = True\n",
    "                        break\n",
    "                    elif str1 == str3 and check_word_in_predicted_entity(str2_raw.lower(), str4_raw.lower()):\n",
    "                        partial_match_found = True\n",
    "                        break\n",
    "                    elif check_word_in_predicted_entity(str1_raw.lower(), str4_raw.lower()) and check_word_in_predicted_entity(str2_raw.lower(), str3_raw.lower()):\n",
    "                        partial_match_found = True\n",
    "                        break\n",
    "                    elif check_word_in_predicted_entity(str1_raw.lower(), str4_raw.lower()) and str2 == str3:\n",
    "                        partial_match_found = True\n",
    "                        break\n",
    "                    elif str1 == str4 and check_word_in_predicted_entity(str2_raw.lower(), str3_raw.lower()):\n",
    "                        partial_match_found = True\n",
    "                        break\n",
    "                    \n",
    "            if match_found:\n",
    "                TP += 1\n",
    "            if partial_match_found:\n",
    "                TP += 1\n",
    "               \n",
    "\n",
    "        interaction_processed_unique = interaction_processed.drop_duplicates()\n",
    "        No_of_original_P_interaction_pairs = (interaction_processed_unique['Fold'] == fold).sum()\n",
    "        \n",
    "        \n",
    "            \n",
    "        if TP > No_of_original_P_interaction_pairs:\n",
    "            TP = No_of_original_P_interaction_pairs\n",
    "        print(\"predicted_pairs, original_P_interaction_pairs, TP:\",No_of_predicted_interaction_pairs, No_of_original_P_interaction_pairs, TP)\n",
    "          \n",
    "            \n",
    "        Partial_Precision = (TP)/No_of_predicted_interaction_pairs\n",
    "        Partial_Recall = (TP)/ No_of_original_P_interaction_pairs  \n",
    "        Partial_F1 = 0 \n",
    "\n",
    "        if Partial_Precision + Partial_Recall > 0:\n",
    "          Partial_F1 = 2 * (Partial_Precision * Partial_Recall)/(Partial_Precision + Partial_Recall)\n",
    "                                           \n",
    "        with open(output_path, \"a\") as f:\n",
    "          print (data_path, fold,',', \"{:.4f}\".format(Partial_Precision), ',', \"{:.4f}\".format(Partial_Recall), ',',\"{:.4f}\".format(Partial_F1), ',', skipped_lines, file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e4cb6f-c4d7-4dc5-9e61-999ec21ab57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_interactions(dataset, base_path_output, model_engine, temperature, prompt_type, run_count=10):\n",
    "\n",
    "    runs = {i: [f'{i}_fold{j}.txt' for j in range(1, 11)] for i in range(1, 11)}\n",
    "\n",
    "    output_paths = {\n",
    "        i: f'{base_path_output}/Evaluation_{dataset}_T{temperature}_{prompt_type}_Run{i}.csv'\n",
    "        for i in range(1, run_count + 1)\n",
    "    }\n",
    "\n",
    "\n",
    "    implementation_base_path_output_runs = {}\n",
    "\n",
    "    implementation_base_path_output_runs = {\n",
    "        i: output_path(i, dataset, temperature, prompt_type, model_engine) for i in range(1, 11)\n",
    "    }\n",
    "\n",
    "    if dataset == \"LLL\":\n",
    "        interaction = pd.read_csv(\"Datasets/LLL/LLL_ALL_P_interaction_10fold.csv\", sep=',')     ### Careful ::::: Run No\n",
    "        interaction = interaction.rename(columns={'Sentence_ID': 'sen_id'})\n",
    "        # Step 1: Sort 'E1' and 'E2' columns within each row to create a new column 'Sorted_Entities'\n",
    "        interaction['Sorted_Entities'] = interaction.apply(lambda row: tuple(sorted([row['E1'], row['E2']])), axis=1)\n",
    "\n",
    "        # Step 2: Drop duplicates based on 'Sentence_ID' and 'Sorted_Entities'\n",
    "        interaction_processed = interaction.drop_duplicates(subset=['sen_id', 'Sorted_Entities'], keep='first')\n",
    "\n",
    "        # Step 3: Drop the temporary 'Sorted_Entities' column\n",
    "        interaction_processed = interaction_processed.drop(columns=['Sorted_Entities'])\n",
    "\n",
    "        interaction_processed = interaction_processed.reset_index(drop=True)\n",
    "\n",
    "        print(len(interaction))\n",
    "        print(len(interaction_processed))\n",
    "\n",
    "\n",
    "    elif dataset == \"HPRD50\":\n",
    "        #base_path = Implementation_base_path_output\n",
    "        interaction = pd.read_csv(\"Datasets/HPRD50/interaction_processed_10fold.csv\", sep=',')     ### Careful ::::: Run No\n",
    "        interaction = interaction[interaction[\"isValid\"] == True]\n",
    "        interaction = interaction.rename(columns={'Sentence_ID': 'sen_id'})\n",
    "\n",
    "        # Step 1: Sort 'E1' and 'E2' columns within each row to create a new column 'Sorted_Entities'\n",
    "        interaction['Sorted_Entities'] = interaction.apply(lambda row: tuple(sorted([row['E1'], row['E2']])), axis=1)\n",
    "\n",
    "        # Step 2: Drop duplicates based on 'Sentence_ID' and 'Sorted_Entities'\n",
    "        interaction_processed = interaction.drop_duplicates(subset=['sen_id', 'Sorted_Entities'], keep='first')\n",
    "\n",
    "        # Step 3: Drop the temporary 'Sorted_Entities' column\n",
    "        interaction_processed = interaction_processed.drop(columns=['Sorted_Entities'])\n",
    "\n",
    "        interaction_processed = interaction_processed.reset_index(drop=True)\n",
    "\n",
    "        print(\"\\n\\n\\ninteraction:\",len(interaction))\n",
    "        print(\"\\n\\n\\ninteraction processed:\",len(interaction_processed))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    elif dataset == \"IEPA\":\n",
    "        df_sentence_dictionary = pd.read_csv(\"Datasets/IEPA/sentence_dictionary_10fold.csv\", sep=',')  \n",
    "        interaction = pd.read_csv(\"Datasets/IEPA/IEPA_all_PN_interaction_final.csv\", sep=',')     ### Careful ::::: Run No\n",
    "        interaction_processed = interaction[interaction[\"isValid\"] == True]\n",
    "        df_sentence_dictionary = df_sentence_dictionary.rename(columns={'Sentence_ID': 'sen_id', 'Fold_no': 'Fold'})\n",
    "        # drop the 'C' column\n",
    "        df_sentence_dictionary = df_sentence_dictionary.drop('Sentence', axis=1)\n",
    "        interaction_processed = pd.merge(interaction_processed, df_sentence_dictionary, on='sen_id', how='inner')\n",
    "\n",
    "        # Step 1: Sort 'E1' and 'E2' columns within each row to create a new column 'Sorted_Entities'\n",
    "        interaction_processed['Sorted_Entities'] = interaction_processed.apply(lambda row: tuple(sorted([row['E1'], row['E2']])), axis=1)\n",
    "        # Step 2: Drop duplicates based on 'Sentence_ID' and 'Sorted_Entities'\n",
    "        interaction_processed = interaction_processed.drop_duplicates(subset=['sen_id', 'Sorted_Entities'], keep='first')\n",
    "        # Step 3: Drop the temporary 'Sorted_Entities' column\n",
    "        interaction_processed = interaction_processed.drop(columns=['Sorted_Entities'])\n",
    "        interaction_processed = interaction_processed.reset_index(drop=True)\n",
    "        print(\"\\n\\n\\ninteraction:\",len(interaction))\n",
    "        print(\"\\n\\n\\ninteraction processed:\",len(interaction_processed))    \n",
    "    else:\n",
    "        interaction_processed = 'Null'\n",
    "        print(\"Sorry the true PPI for this dataset is not available now.\")\n",
    "\n",
    "\n",
    "    for i in range(1, 11):\n",
    "        print(implementation_base_path_output_runs)\n",
    "        print(runs)\n",
    "        print(output_paths)\n",
    "        evaluation(implementation_base_path_output_runs[i], runs[i], output_paths[i], interaction_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d56e0-026d-4e06-b1bf-9a9564dec7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_evaluation(folder_path, temperature):\n",
    "    # Create an empty DataFrame to store the results\n",
    "    results_df = pd.DataFrame(columns=[\"filename\", \"average precision\", \"average recall\", \"average f1 score\", \"total bad lines\"])\n",
    "\n",
    "    # Loop through the CSV files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(os.path.join(folder_path, filename), header=None, names=[\"filename\", \"precision\", \"recall\", \"f1 score\", \"total bad lines\"])\n",
    "\n",
    "            # Calculate averages and total bad lines\n",
    "            avg_precision = df[\"precision\"].mean() * 100\n",
    "            avg_recall = df[\"recall\"].mean() * 100\n",
    "            avg_f1_score = df[\"f1 score\"].mean() * 100\n",
    "            total_bad_lines = df[\"total bad lines\"].sum()\n",
    "\n",
    "            # Append results to the DataFrame\n",
    "            results_df = results_df.append({\n",
    "                \"filename\": filename,\n",
    "                \"average precision\": avg_precision,\n",
    "                \"average recall\": avg_recall,\n",
    "                \"average f1 score\": avg_f1_score,\n",
    "                \"total bad lines\": total_bad_lines\n",
    "            }, ignore_index=True)\n",
    "\n",
    "    # Extract \"T_value\" from filenames\n",
    "    results_df[\"T_value\"] = results_df[\"filename\"].str.extract(\"T(\\d+\\.\\d+)\")\n",
    "\n",
    "    # Select relevant columns for a shortened DataFrame\n",
    "    shortened_df = results_df[[\"T_value\", \"average precision\", \"average recall\", \"average f1 score\", \"total bad lines\"]].sort_values(by=\"T_value\")\n",
    "\n",
    "    # Calculate overall averages\n",
    "    avg_precision = round(shortened_df['average precision'].mean(), 2)\n",
    "    avg_recall = round(shortened_df['average recall'].mean(), 2)\n",
    "    avg_f1 = round(shortened_df['average f1 score'].mean(), 2)\n",
    "    total_bad_lines = shortened_df['total bad lines'].mean()\n",
    "\n",
    "    # Append a row with the overall averages\n",
    "    new_row = {'T_value': temperature, 'average precision': avg_precision, 'average recall': avg_recall, 'average f1 score': avg_f1, \"total bad lines\": total_bad_lines}\n",
    "    shortened_df = shortened_df.append(new_row, ignore_index=True)\n",
    "\n",
    "    # Save the summarized data to a CSV file\n",
    "    filepath = os.path.join(folder_path, 'Average_Score.csv')\n",
    "    shortened_df.to_csv(filepath, index=False, float_format='%.2f')\n",
    "\n",
    "    # Return the summarized DataFrame\n",
    "    return shortened_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9b117d-d0aa-4163-a566-ee3fdf528cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    prompt_types = ['BASE', 'WD', 'WND']\n",
    "    model_engines = [\"gpt-3.5-turbo-0613\", \"gpt-4-0613\"]\n",
    "    datasets = [\"LLL\", \"HPRD50\", \"IEPA\"]\n",
    "    temperature = 0.0\n",
    "\n",
    "    for dataset in datasets:\n",
    "        for model_engine in model_engines:\n",
    "            for prompt_type in prompt_types:\n",
    "                \n",
    "                print(f\"Processing: Dataset={dataset}, Model={model_engine}, Prompt={prompt_type}, Temperature={temperature}\")\n",
    "                base_path_output = f'Output/{model_engine}_Final_Prompts_{temperature}/Prompt{prompt_type}/{dataset}/'\n",
    "\n",
    "                # Process interactions\n",
    "                interaction_processed = process_interactions(dataset, base_path_output, model_engine, temperature, prompt_type)\n",
    "                print(\"Processed interactions:\", interaction_processed)\n",
    "                # Summarize evaluation\n",
    "                summarized_data = summarize_evaluation(base_path_output, temperature)\n",
    "                print(\"Summarized data:\\n\", summarized_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openaienv",
   "language": "python",
   "name": "openaienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
