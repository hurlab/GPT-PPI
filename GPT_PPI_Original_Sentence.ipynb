{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257cec86-5e0e-404d-a2ed-a26096b8aca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import os \n",
    "import io\n",
    "import openai\n",
    "from transformers.models.imagegpt.modeling_imagegpt import IMAGEGPT_INPUTS_DOCSTRING\n",
    "from transformers import GPT2Tokenizer\n",
    "import pandas as pd \n",
    "from pandas.io import json\n",
    "from numpy import nan\n",
    "import time\n",
    "import csv\n",
    "import shutil\n",
    "import datetime\n",
    "import pytz\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e05bed-73e5-40dc-bd5f-d6883967032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe13c0e-d192-42fb-bc91-4dbafc0ce55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your API key should go here\n",
    "openai.api_key = \"OPENAI_API_KEY\"\n",
    "section_no = \"Final_Prompts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d06b2d8-fad3-4465-9c41-40e693cfe12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate All the variations \n",
    "import itertools\n",
    "import random\n",
    "\n",
    "def get_combinations(prompt_paths, model_engines, datasets, folds_LLL, folds_HPRD50, folds_IEPA):\n",
    "    combinations = []\n",
    "\n",
    "    # Create dictionary for dataset-specific folds\n",
    "    dataset_folds = {\n",
    "        'LLL': folds_LLL,\n",
    "        'HPRD50': folds_HPRD50,\n",
    "        'IEPA': folds_IEPA\n",
    "    }\n",
    "\n",
    "    # Iterate over each dataset to get the dataset-specific folds and prompts\n",
    "    for dataset in datasets:\n",
    "        current_folds = dataset_folds[dataset]\n",
    "        for combination in itertools.product(prompt_paths, model_engines, [dataset], current_folds):\n",
    "            combinations.append(combination)\n",
    "            \n",
    "    # Shuffle the list of combinations\n",
    "    random.shuffle(combinations)\n",
    "    return combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f0dfe2-8c52-4b88-835c-16229d263039",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get user input\n",
    "def user_input(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            query = file.read()\n",
    "            file_name = file_path.split('/')[-1]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{file_path} not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        \n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c314c0-42ba-4b97-826e-787a3320867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define output path\n",
    "def output_path(Run_no, dataset, temperature, prompt_type, model_engine):\n",
    "    base = \"Output/\" + model_engine + \"_\"+section_no+\"_\" + str(temperature) + \"/Prompt\" + str(prompt_type) +\"/\"+dataset + \"/\"\n",
    "    extension_path = dataset + \"_T\" + str(temperature) + \"_\" + prompt_type + \"_Run\" + str(Run_no)+'/'\n",
    "    Implementation_base_path_output = os.path.join(base, extension_path)\n",
    "    os.makedirs(Implementation_base_path_output, exist_ok=True)\n",
    "    return Implementation_base_path_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a38a74-3097-4402-8bd9-2890394a0bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count number of tokens\n",
    "def count_input_tokens(folder_path):\n",
    "  # Initialize GPT2 tokenizer\n",
    "  tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "  # Get list of file names in folder\n",
    "  file_names = os.listdir(folder_path)\n",
    "  input_max_token = 0\n",
    "\n",
    "  # Loop through each file and calculate number of tokens\n",
    "  for file_name in file_names:\n",
    "      # Check if file is a text file\n",
    "      if file_name.endswith(\".txt\"):\n",
    "          # Read file contents\n",
    "          with open(os.path.join(folder_path, file_name), \"r\" , encoding='utf-8') as f:\n",
    "              file_contents = f.read()\n",
    "          # Calculate number of tokens\n",
    "          num_tokens = len(tokenizer.encode(file_contents))\n",
    "          if (num_tokens>input_max_token):\n",
    "            input_max_token = num_tokens\n",
    "            \n",
    "  total_max_tokens = input_max_token+200\n",
    "\n",
    "  # Loop through each file and calculate number of tokens\n",
    "  for file_name in file_names:\n",
    "      # Check if file is a text file\n",
    "      if file_name.endswith(\".txt\"):\n",
    "          # Read file contents\n",
    "          with open(os.path.join(folder_path, file_name), \"r\" , encoding='utf-8') as f:\n",
    "              file_contents = f.read()\n",
    "          # Calculate number of tokens\n",
    "          num_tokens = len(tokenizer.encode(file_contents))\n",
    "          total = num_tokens+ total_max_tokens\n",
    "        \n",
    "  return total_max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7e57eb-b6c8-455c-ac07-c5d2825b8803",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ChatGPT Completion\n",
    "def get_completion(BACKOFF_OCCURRED_, model, query, Sentences, max_tokens,  temperature):\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        {query}\n",
    "        {Sentences}\n",
    "        \"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    tries = 0\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "            )\n",
    "            call_time = time.time()\n",
    "            time_f = call_time - start_time\n",
    "            break\n",
    "        except (openai.error.RateLimitError, openai.error.ServiceUnavailableError, openai.error.APIError) as e:\n",
    "            tries += 1\n",
    "            max_backoff = 60  # Example: maximum of 60 seconds\n",
    "            backoff_time =  min(5 + 5*(tries ** 2), max_backoff)\n",
    "            with BACKOFF_OCCURRED_.get_lock():\n",
    "                BACKOFF_OCCURRED_.value = True\n",
    "\n",
    "            time.sleep(backoff_time)\n",
    "            with BACKOFF_OCCURRED_.get_lock():\n",
    "                BACKOFF_OCCURRED_.value = False\n",
    "            print(\"Backoff Released\\n\")\n",
    "    \n",
    "    message = response['choices'][0]['message']['content']\n",
    "    output_token = response['usage']['completion_tokens']\n",
    "    input_token = response['usage']['prompt_tokens']\n",
    "    \n",
    "    now_utc = datetime.datetime.now(pytz.utc)\n",
    "    timezone = pytz.timezone(\"US/Central\")\n",
    "    now_eastern = now_utc.astimezone(timezone)\n",
    "    time_stamp = str(now_eastern)\n",
    "    return message, input_token, output_token, time_f, time_stamp, tries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5461b19-3ef5-49f5-8e13-17f320d5b91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call chatGPT\n",
    "def call_ChatGPT(BACKOFF_OCCURRED_, current_folds, Implementation_base_path_input, Implementation_base_path_output, Run, total_max_tokens, temperature, query, dataset, model):\n",
    "\n",
    "        RETRY_COUNT = 5\n",
    "        input_file_path = os.path.join(Implementation_base_path_input, f'{current_folds}')\n",
    "        output_file_path = os.path.join(Implementation_base_path_output, f'{Run}_{current_folds}')\n",
    "        time_track_path = os.path.join(Implementation_base_path_output, f'{temperature}_time_track.csv')\n",
    "\n",
    "        while BACKOFF_OCCURRED_.value:\n",
    "            print(\"\\nBackoff occurred! Pausing all threads for a set duration...\")\n",
    "            sleep_time = random.randint(1, 5)\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "            \n",
    "        for attempt in range(RETRY_COUNT):\n",
    "            try:\n",
    "                with open(input_file_path) as f:\n",
    "                    Sentences = f.read()\n",
    "                message, input_token, output_token, time_f, time_stamp, tries = get_completion(BACKOFF_OCCURRED_, model, query, Sentences=Sentences, max_tokens=total_max_tokens, temperature=temperature)\n",
    "\n",
    "                last_line = message.strip().split('\\n')[-1]\n",
    "                status = \"Complete\" if \"Done\" in last_line else \"Possibly Incomplete\"\n",
    "\n",
    "                print(f\"Fold ={current_folds}, Run= {Run}, Temperature={temperature}, {status}. output_file_path: {output_file_path}\")\n",
    "\n",
    "                with open(time_track_path, \"a\") as f:\n",
    "                    print(dataset, ',', Run, ',', current_folds, ',', temperature, ',', input_token, ',', output_token, ',', time_f, ',', time_stamp, ',', tries, file=f)\n",
    "\n",
    "                with open(output_file_path, \"w\") as f:\n",
    "                    print(message, file=f)\n",
    "                    \n",
    "                break\n",
    "                \n",
    "            except Exception as e:  # Catch general exceptions\n",
    "                    print(f\"Error occurred: {e}. Retrying {attempt+1}/{RETRY_COUNT}. output_file_path: {output_file_path}\")\n",
    "                    time.sleep(1)  \n",
    "\n",
    "            else:  # This block will be executed if the for loop completed without 'break', i.e., if all attempts failed.\n",
    "                print(f\"All {RETRY_COUNT} retries failed for fold {current_folds} at Run {Run}. output_file_path: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e5c16d-2d96-4380-824a-173263d4b64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#execution code\n",
    "BACKOFF_OCCURRED_ = multiprocessing.Value('b', False)  # 'b' denotes a boolean\n",
    "\n",
    "def execute_code(run_no, prompt_type, model_engine, dataset, current_folds):\n",
    "    global BACKOFF_OCCURRED_\n",
    "    print(run_no, prompt_type, model_engine, dataset, current_folds)\n",
    "    \n",
    "    if prompt_type ==  'BASE':\n",
    "        prompt_path = f\"Prompts/Final_Prompts/P60_S3_{prompt_type}.txt\"\n",
    "    elif prompt_type ==  'WD':\n",
    "        prompt_path = f\"Prompts/Final_Prompts/P60_S3_{prompt_type}_{dataset}.txt\"\n",
    "    elif prompt_type ==  'WND':\n",
    "        prompt_path = f\"Prompts/Final_Prompts/P60_S3_{prompt_type}_{dataset}.txt\"\n",
    "        \n",
    "    temperature = 0.0 \n",
    "    query = user_input(prompt_path)\n",
    "    Implementation_base_path_input = 'Datasets/' + dataset + '/10fold/'\n",
    "    Implementation_base_path_output = output_path(run_no, dataset, temperature, prompt_type, model_engine)\n",
    "    total_max_tokens = count_input_tokens(Implementation_base_path_input)\n",
    "    call_ChatGPT(BACKOFF_OCCURRED_, current_folds, Implementation_base_path_input, Implementation_base_path_output, run_no, total_max_tokens, temperature, query, dataset,model = model_engine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946bafa8-a8ff-4c11-9128-ca37ca0e7e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main function\n",
    "if __name__ == '__main__':\n",
    "    prompt_types = [\n",
    "        'BASE', \n",
    "        'WD',\n",
    "        'WND'\n",
    "    ]\n",
    "    model_engines = [\"gpt-3.5-turbo-0613\", \"gpt-4-0613\"]\n",
    "    datasets = [\"LLL\", \"HPRD50\", \"IEPA\"]\n",
    "    # List for LLL and HPRD50 with 10 folds\n",
    "    folds_LLL = [f\"fold{i}.txt\" for i in range(1, 11)]\n",
    "    folds_HPRD50 = [f\"fold{i}.txt\" for i in range(1, 11)]\n",
    "    # Lists for IEPA with 10 folds , and each fold is split into two parts\n",
    "    folds_IEPA = [f\"fold{i}_part{j}.txt\" for i in range(1, 11) for j in range(1, 4)]\n",
    "\n",
    "    for Run_no in range(1, 11):\n",
    "        all_combinations = get_combinations(prompt_types, model_engines, datasets, folds_LLL, folds_HPRD50, folds_IEPA)\n",
    "        print(f\"Total number of combinations: {len(all_combinations)}\")\n",
    "        args = [(Run_no,) + combo for combo in all_combinations]\n",
    "        no_of_workers = 20\n",
    "        with multiprocessing.Pool(no_of_workers) as pool:\n",
    "            pool.starmap(execute_code, args)\n",
    "            time.sleep(1)\n",
    "            \n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openaienv",
   "language": "python",
   "name": "openaienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
