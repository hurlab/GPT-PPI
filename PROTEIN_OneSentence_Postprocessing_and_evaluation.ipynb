{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53763f8a-f18f-493c-9ae1-cb675bff36ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_PROTEIN(data_path, dataset, output_path, run):\n",
    "    run_standard = ['fold1.txt', 'fold2.txt', 'fold3.txt','fold4.txt', 'fold5.txt', 'fold6.txt', 'fold7.txt', 'fold8.txt', 'fold9.txt','fold10.txt']\n",
    "    data_path_standard = f'Datasets/PROTEIN_DATA/{dataset}/PROTEIN_all/standard/'\n",
    "    for f in range(10):\n",
    "        fold = f+1\n",
    "        \n",
    "        \n",
    "        \n",
    "        df_predicted_processed = pd.read_csv(os.path.join(data_path, run[f]), sep = ',', on_bad_lines= 'skip')\n",
    "        \n",
    "        \n",
    "        df_predicted_processed = df_predicted_processed.dropna()\n",
    "        df_predicted_processed = df_predicted_processed.drop(df_predicted_processed[df_predicted_processed['Sentence ID'] == 'Done'].index)\n",
    "        df_predicted_processed.columns = ['Sentence ID','PPI']\n",
    "        interaction_processed = pd.read_csv(os.path.join(data_path_standard, run_standard[f]), sep = '\\t')\n",
    "        #interaction_processed = interaction_processed [\n",
    "        y = interaction_processed['isValid']\n",
    "        #y = convert_bool_to_string(y)\n",
    "        \n",
    "        y_pred = df_predicted_processed ['PPI'].astype(bool)\n",
    "        #y_pred = df_predicted_processed['PPI'].apply(lambda x: True if x == 'TRUE' else False)\n",
    "        #print(y)\n",
    "        #print(y_pred)\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        #precision, recall, f1 = calculate_metrics(y, y_pred)\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        precision = precision_score(y, y_pred)\n",
    "        recall = recall_score(y, y_pred)\n",
    "        f1 = f1_score(y, y_pred)\n",
    "        \n",
    "        print(\"Fold = \",fold)\n",
    "        # Print performance metrics\n",
    "        #print(f\"Accuracy: {accuracy}\")\n",
    "        print(f\"Precision: {precision}\")\n",
    "        print(f\"Recall: {recall}\")\n",
    "        print(f\"F1 Score: {f1}\")\n",
    "        \n",
    "        print(data_path)\n",
    "        \n",
    "        with open(output_path, \"a\") as f:\n",
    "          print (data_path, fold,',', \"{:.4f}\".format(precision), ',', \"{:.4f}\".format(recall), ',',\"{:.4f}\".format(f1), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2954ac88-0ab4-46a2-ab49-2d4774ebfe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "section_no = \"OneSentence_PROTEIN_Prompts\"\n",
    "def output_path(Run_no, dataset, temperature, prompt_no, model_engine):\n",
    "    base = \"Output/\" + model_engine + \"_\"+section_no+\"_\" + str(temperature) + \"/\" + str(prompt_no) +\"/\"+dataset + \"/\"\n",
    "    extension_path = dataset + \"_T\" + str(temperature) + \"_\" + prompt_no + \"_Run\" + str(Run_no)+'/'\n",
    "\n",
    "    # Join the base path and extension path\n",
    "    Implementation_base_path_output = os.path.join(base, extension_path)\n",
    "    os.makedirs(Implementation_base_path_output, exist_ok=True)\n",
    "    #print(Implementation_base_path_output)\n",
    "    return Implementation_base_path_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5a80c9-c20c-4a9e-a187-7f4173671f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Define the parameters\n",
    "temperatures = [0.0]  # Add more temperatures if needed\n",
    "model_engines = [\"gpt-4-0613\", \"gpt-3.5-turbo-0613\"]\n",
    "datasets = [\"LLL\", \"IEPA\", \"HPRD50\"]\n",
    "prompt_no = 15\n",
    "\n",
    "# Iterate over each combination of model engine, dataset, and temperature\n",
    "for model_engine in model_engines:\n",
    "    for dataset in datasets:\n",
    "        for temperature in temperatures:\n",
    "            for R in range(1, 11):\n",
    "                query_no = f\"Prompt{prompt_no}_OneSentence\"\n",
    "                # Define the runs\n",
    "                runs = [f'{R}_fold{i}.txt' for i in range(1, 11)]\n",
    "                # Construct the output path\n",
    "                output_path_R = f'Output/{model_engine}_OneSentence_PROTEIN_Prompts_{temperature}/{query_no}/{dataset}/Evaluation_{dataset}_T{temperature}_{query_no}_Run{R}.csv'\n",
    "                Implementation_base_path_output_Run = output_path(R, dataset, temperature, query_no, model_engine)\n",
    "                evaluation_PROTEIN(Implementation_base_path_output_Run, dataset, output_path_R, runs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea5a981-3a21-4cba-b33e-48be5fad0e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the parameters\n",
    "model_engines = [\"gpt-4-0613\", \"gpt-3.5-turbo-0613\"]\n",
    "datasets = [\"LLL\", \"IEPA\", \"HPRD50\"]\n",
    "\n",
    "for model_engine in model_engines:\n",
    "    for dataset in datasets:\n",
    "        for prompt_no in range(15, 16):\n",
    "            # Define the folder path containing the CSV files\n",
    "            folder_path = f\"Output/{model_engine}_OneSentence_PROTEIN_Prompts_0.0/Prompt{prompt_no}_OneSentence/{dataset}/\"\n",
    "\n",
    "            # Create an empty dataframe to store the results\n",
    "            results_df = pd.DataFrame(columns=[\"filename\", \"average precision\", \"average recall\", \"average f1 score\"])\n",
    "\n",
    "            # Loop through the CSV files in the folder\n",
    "            for filename in os.listdir(folder_path):\n",
    "                if filename.endswith(\".csv\"):\n",
    "                    # Read the CSV file into a dataframe\n",
    "                    df = pd.read_csv(os.path.join(folder_path, filename), header=None, names=[\"filename\", \"precision\", \"recall\", \"f1 score\"])\n",
    "\n",
    "                    # Calculate the average precision, recall, and f1 score for each file\n",
    "                    avg_precision = df[\"precision\"].mean()\n",
    "                    avg_recall = df[\"recall\"].mean()\n",
    "                    avg_f1_score = df[\"f1 score\"].mean()\n",
    "\n",
    "                    # Add the results to the results dataframe\n",
    "                    results_df = results_df.append({\n",
    "                        \"filename\": filename,\n",
    "                        \"average precision\": avg_precision,\n",
    "                        \"average recall\": avg_recall,\n",
    "                        \"average f1 score\": avg_f1_score\n",
    "                    }, ignore_index=True)\n",
    "\n",
    "            # Extract the \"T0.0\"-like part of the filenames and store them in a new column called \"T_value\"\n",
    "            results_df[\"T_value\"] = results_df[\"filename\"].str.extract(\"T(\\d+\\.\\d+)\")\n",
    "\n",
    "            # Create a new dataframe with only the shortened filename and the average precision, recall, and f1 score columns\n",
    "            shortened_df = results_df[[\"T_value\", \"average precision\", \"average recall\", \"average f1 score\"]]\n",
    "\n",
    "            # Sort the shortened dataframe by the \"T_value\" column\n",
    "            shortened_df = shortened_df.sort_values(by=\"T_value\")\n",
    "\n",
    "            # Convert average precision, recall, and F1 score to percentages and round them\n",
    "            shortened_df[['average precision', 'average recall', 'average f1 score']] *= 100\n",
    "            shortened_df[['average precision', 'average recall', 'average f1 score']] = shortened_df[['average precision', 'average recall', 'average f1 score']].round(2)\n",
    "\n",
    "            # Calculate and print the average scores\n",
    "            avg_precision = round(shortened_df['average precision'].mean(), 2)\n",
    "            avg_recall = round(shortened_df['average recall'].mean(), 2)\n",
    "            avg_f1 = round(shortened_df['average f1 score'].mean(), 2)\n",
    "            print(f\"Model: {model_engine}, Dataset: {dataset}, Average Precision: {avg_precision}, Average Recall: {avg_recall}, Average F1 Score: {avg_f1}\")\n",
    "\n",
    "            # Append the average scores to the DataFrame\n",
    "            new_row = {'T_value': 'Average', 'average precision': avg_precision, 'average recall': avg_recall, 'average f1 score': avg_f1}\n",
    "            shortened_df = shortened_df.append(new_row, ignore_index=True)\n",
    "\n",
    "            # Save the DataFrame as a CSV file\n",
    "            filepath = os.path.join(folder_path, 'Average_Score.csv')\n",
    "            shortened_df.to_csv(filepath, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openaienv",
   "language": "python",
   "name": "openaienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
