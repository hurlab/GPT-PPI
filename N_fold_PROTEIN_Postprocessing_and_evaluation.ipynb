{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1a34a6-eaf6-4144-9b95-a9e745975929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def add_header_to_txt_files_in_structure(base_directory):\n",
    "    try:\n",
    "        # Define patterns for the directory structure\n",
    "        patterns = [\n",
    "            \"gpt-4-0613_N_fold_PROTEIN_Prompts_0.0\",\n",
    "            \"gpt-3.5-turbo-0613_N_fold_PROTEIN_Prompts_0.0\"\n",
    "        ]\n",
    "\n",
    "        for i in range(1, 17):\n",
    "            for j in range(1, 11):\n",
    "                for pattern in patterns:\n",
    "                    # Construct directory paths\n",
    "                    dir_paths = [\n",
    "                        os.path.join(base_directory, pattern, f\"Prompt{i}\", \"LLL\", f\"LLL_T0.0_Prompt{i}_Run{j}\"),\n",
    "                        os.path.join(base_directory, pattern, f\"Prompt{i}\", \"HPRD50\", f\"HPRD50_T0.0_Prompt{i}_Run{j}\"),\n",
    "                        os.path.join(base_directory, pattern, f\"Prompt{i}\", \"IEPA\", f\"IEPA_T0.0_Prompt{i}_Run{j}\")\n",
    "                    ]\n",
    "\n",
    "                    for dir_path in dir_paths:\n",
    "                        if os.path.exists(dir_path):\n",
    "                            # Process each .txt file in the directory\n",
    "                            for file in os.listdir(dir_path):\n",
    "                                if file.endswith('.txt'):\n",
    "                                    file_path = os.path.join(dir_path, file)\n",
    "                                    with open(file_path, 'r') as f:\n",
    "                                        content = f.readlines()\n",
    "\n",
    "                                    # Check if the first line is the expected header\n",
    "                                    if not content or not content[0].strip().startswith(\"Sentence ID,PPI\"):\n",
    "                                        content.insert(0, \"Sentence ID,PPI\\n\")\n",
    "                                        print(\"Header added to\",file_path)\n",
    "                                        with open(file_path, 'w') as f:\n",
    "                                            f.writelines(content)\n",
    "\n",
    "        return \"Header added successfully where necessary.\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "# Example usage\n",
    "base_directory = 'Output/'  # Ensure this is correctly indented\n",
    "add_header_to_txt_files_in_structure(base_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53763f8a-f18f-493c-9ae1-cb675bff36ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_PROTEIN(data_path, dataset, output_path, run):\n",
    "    if dataset == \"LLL\":\n",
    "        run_standard = [f\"fold{i}.csv\" for i in range(1, 37)]\n",
    "    if dataset == \"HPRD50\":\n",
    "        run_standard = [f\"fold{i}.csv\" for i in range(1, 22)]\n",
    "    if dataset == \"IEPA\":\n",
    "        run_standard = [f\"fold{i}.csv\" for i in range(1, 16)]\n",
    "    data_path_standard = f'Datasets/PROTEIN_DATA/{dataset}/N_fold/N_fold_csv/'\n",
    "    for f in range(10):\n",
    "        fold = f+1\n",
    "\n",
    "        df_predicted_processed = pd.read_csv(os.path.join(data_path, run[f]), sep = ',', on_bad_lines= 'skip')\n",
    "        \n",
    "        \n",
    "        df_predicted_processed = df_predicted_processed.dropna()\n",
    "        df_predicted_processed = df_predicted_processed.drop(df_predicted_processed[df_predicted_processed['Sentence ID'] == 'Done'].index)\n",
    "        df_predicted_processed.columns = ['Sentence ID','PPI']\n",
    "        \n",
    "        # Check data type\n",
    "        print(df_predicted_processed['PPI'].dtype)\n",
    "\n",
    "        # Check unique values for hidden characters or formats\n",
    "        print(df_predicted_processed['PPI'].unique())\n",
    "        \n",
    "        interaction_processed = pd.read_csv(os.path.join(data_path_standard, run_standard[f]), sep = ',')\n",
    "        #interaction_processed = interaction_processed [\n",
    "        y = interaction_processed['isValid']\n",
    "\n",
    "        y_pred = df_predicted_processed['PPI'].astype(bool)\n",
    "\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        precision = precision_score(y, y_pred)\n",
    "        recall = recall_score(y, y_pred)\n",
    "        f1 = f1_score(y, y_pred)\n",
    "        \n",
    "        print(\"Fold = \",fold)\n",
    "        print(f\"Precision: {precision}\")\n",
    "        print(f\"Recall: {recall}\")\n",
    "        print(f\"F1 Score: {f1}\")\n",
    "        \n",
    "        print(data_path)\n",
    "        \n",
    "        with open(output_path, \"a\") as f:\n",
    "          print (data_path, fold,',', \"{:.4f}\".format(precision), ',', \"{:.4f}\".format(recall), ',',\"{:.4f}\".format(f1), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2954ac88-0ab4-46a2-ab49-2d4774ebfe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "section_no = \"N_fold_PROTEIN_Prompts\"\n",
    "def output_path(Run_no, dataset, temperature, prompt_no, model_engine):\n",
    "    base = \"Output/\" + model_engine + \"_\"+section_no+\"_\" + str(temperature) + \"/\" + str(prompt_no) +\"/\"+dataset + \"/\"\n",
    "    extension_path = dataset + \"_T\" + str(temperature) + \"_\" + prompt_no + \"_Run\" + str(Run_no)+'/'\n",
    "\n",
    "    # Join the base path and extension path\n",
    "    Implementation_base_path_output = os.path.join(base, extension_path)\n",
    "    os.makedirs(Implementation_base_path_output, exist_ok=True)\n",
    "\n",
    "    return Implementation_base_path_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86b4c1c-036e-41aa-bf59-a506b73cd4cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5a80c9-c20c-4a9e-a187-7f4173671f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "temperature = 0.0\n",
    "model_engines = [\"gpt-3.5-turbo-0613\", \"gpt-4-0613\"]\n",
    "datasets = [\"LLL\", \"HPRD50\", \"IEPA\"]\n",
    "\n",
    "prompt_no = 15\n",
    "for model_engine in model_engines:\n",
    "    for dataset in datasets:\n",
    "        for R in range(1, 11):\n",
    "            query_no = f\"Prompt{prompt_no}\"\n",
    "\n",
    "            if dataset == \"LLL\":\n",
    "                run = [f'{R}_fold{i}.txt' for i in range(1, 37)]\n",
    "            elif dataset == \"HPRD50\":\n",
    "                run = [f'{R}_fold{i}.txt' for i in range(1, 22)]\n",
    "            elif dataset == \"IEPA\":\n",
    "                run = [f'{R}_fold{i}.txt' for i in range(1, 16)]\n",
    "\n",
    "            # Construct the output path\n",
    "            output_path_R = f'Final_outputs_all/{model_engine}_N_fold_{dataset}_Prompts_0.0/{query_no}/{dataset}/Evaluation_{dataset}_T{temperature}_{query_no}_Run{R}.csv'\n",
    "            Implementation_base_path_output_Run = output_path(R, dataset, temperature, query_no, model_engine)\n",
    "            evaluation_PROTEIN(Implementation_base_path_output_Run, dataset, output_path_R, run)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea5a981-3a21-4cba-b33e-48be5fad0e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "model_engines = [\"gpt-3.5-turbo-0613\", \"gpt-4-0613\"]\n",
    "datasets = [\"LLL\", \"HPRD50\", \"IEPA\"]\n",
    "\n",
    "for model_engine in model_engines:\n",
    "    for dataset in datasets:\n",
    "        for prompt_no in range(15, 16):  # Adjust the range as needed\n",
    "\n",
    "            # Define the folder path containing the CSV files\n",
    "            folder_path = f\"Output/{model_engine}_N_fold_PROTEIN_Prompts_0.0/Prompt{prompt_no}/{dataset}/\"\n",
    "\n",
    "            # Create an empty dataframe to store the results\n",
    "            results_df = pd.DataFrame(columns=[\"filename\", \"average precision\", \"average recall\", \"average f1 score\"])\n",
    "\n",
    "            # Loop through the CSV files in the folder\n",
    "            for filename in os.listdir(folder_path):\n",
    "                if filename.endswith(\".csv\"):\n",
    "                    # Read the CSV file into a dataframe\n",
    "                    df = pd.read_csv(os.path.join(folder_path, filename), header=None, names=[\"filename\", \"precision\", \"recall\", \"f1 score\"])\n",
    "\n",
    "                    # Calculate the average precision, recall, and f1 score for each file\n",
    "                    avg_precision = df[\"precision\"].mean()\n",
    "                    avg_recall = df[\"recall\"].mean()\n",
    "                    avg_f1_score = df[\"f1 score\"].mean()\n",
    "\n",
    "                    # Add the results to the results dataframe\n",
    "                    results_df = results_df.append({\n",
    "                        \"filename\": filename,\n",
    "                        \"average precision\": avg_precision,\n",
    "                        \"average recall\": avg_recall,\n",
    "                        \"average f1 score\": avg_f1_score\n",
    "                    }, ignore_index=True)\n",
    "\n",
    "            # Sort and process the dataframe\n",
    "            results_df[\"T_value\"] = results_df[\"filename\"].str.extract(\"T(\\d+\\.\\d+)\")\n",
    "            shortened_df = results_df[[\"T_value\", \"average precision\", \"average recall\", \"average f1 score\"]].sort_values(by=\"T_value\")\n",
    "            shortened_df[['average precision', 'average recall', 'average f1 score']] *= 100\n",
    "            shortened_df[['average precision', 'average recall', 'average f1 score']] = shortened_df[['average precision', 'average recall', 'average f1 score']].round(2)\n",
    "\n",
    "            # Calculate and print the average scores\n",
    "            avg_precision = round(shortened_df['average precision'].mean(), 2)\n",
    "            avg_recall = round(shortened_df['average recall'].mean(), 2)\n",
    "            avg_f1 = round(shortened_df['average f1 score'].mean(), 2)\n",
    "            print(f\"Model Engine: {model_engine}, Dataset: {dataset}, Prompt: {prompt_no}\")\n",
    "            print(\"Average Precision:\", avg_precision)\n",
    "            print(\"Average Recall:\", avg_recall)\n",
    "            print(\"Average F1 Score:\", avg_f1)\n",
    "\n",
    "            # Append the new row to the DataFrame\n",
    "            new_row = {'T_value': 'Average', 'average precision': avg_precision, 'average recall': avg_recall, 'average f1 score': avg_f1}\n",
    "            shortened_df = shortened_df.append(new_row, ignore_index=True)\n",
    "\n",
    "            # Save the results\n",
    "            filepath = os.path.join(folder_path, 'Average_Score.csv')\n",
    "            shortened_df.to_csv(filepath, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openaienv",
   "language": "python",
   "name": "openaienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
