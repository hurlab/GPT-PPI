{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1667a0b5-dc83-416b-b4bb-dbe9b5096ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import io\n",
    "import openai\n",
    "from transformers.models.imagegpt.modeling_imagegpt import IMAGEGPT_INPUTS_DOCSTRING\n",
    "from transformers import GPT2Tokenizer\n",
    "import pandas as pd \n",
    "from pandas.io import json\n",
    "from numpy import nan\n",
    "import time\n",
    "import csv\n",
    "import shutil\n",
    "import datetime\n",
    "import pytz\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f898ea-5842-4282-83d9-ab28c47f7d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ec6eb3-4bc4-4237-98a3-427cd0af2ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"OPENAI_API_KEY\"\n",
    "section_no = \"PROTEIN_Prompts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8f5b2f-c3fa-4588-9da0-9d21c0999a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate All the variations \n",
    "def get_combinations(prompt_paths, model_engines, datasets, folds_LLL, folds_HPRD50, folds_IEPA):\n",
    "    combinations = []\n",
    "    dataset_folds = {\n",
    "        'LLL': folds_LLL,\n",
    "        'HPRD50': folds_HPRD50,\n",
    "        'IEPA': folds_IEPA\n",
    "    }\n",
    "    for dataset in datasets:\n",
    "        current_folds = dataset_folds[dataset]\n",
    "        for combination in itertools.product(prompt_paths, model_engines, [dataset], current_folds):\n",
    "            combinations.append(combination)\n",
    "    random.shuffle(combinations)\n",
    "    return combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86b17ff-e470-48b4-abf0-4aa232e21cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_input(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            query = file.read()\n",
    "            file_name = file_path.split('/')[-1]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{file_path} not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e08387-1cc8-4367-973c-983fdda3eb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_path(Run_no, dataset, temperature, prompt_no, model_engine):\n",
    "    base = \"Output/\" + model_engine + \"_\"+section_no+\"_\" + str(temperature) + \"/\" + str(prompt_no) +\"/\"+dataset + \"/\"\n",
    "    extension_path = dataset + \"_T\" + str(temperature) + \"_\" + prompt_no + \"_Run\" + str(Run_no)+'/'\n",
    "    Implementation_base_path_output = os.path.join(base, extension_path)\n",
    "    os.makedirs(Implementation_base_path_output, exist_ok=True)\n",
    "    return Implementation_base_path_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e52353a-0c1e-4bc1-9853-d9f82e60dc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_input_tokens(folder_path):\n",
    "  tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "  file_names = os.listdir(folder_path)\n",
    "  max_num_input_lines = 0\n",
    "\n",
    "  # Loop through each file and calculate number of tokens\n",
    "  for file_name in file_names:\n",
    "      if file_name.endswith(\".txt\"):\n",
    "          with open(os.path.join(folder_path, file_name), \"r\" , encoding='utf-8') as f:\n",
    "              file_contents = f.read()\n",
    "          with open(os.path.join(folder_path, file_name), \"r\" , encoding='utf-8') as fp:\n",
    "              num_input_line = len(fp.readlines())\n",
    "              print(\"num_input_line:\",num_input_line)\n",
    "          num_tokens = len(tokenizer.encode(file_contents))\n",
    "          if (num_input_line>max_num_input_lines):\n",
    "            max_num_input_lines = num_input_line\n",
    "          print(f\"{file_name}: {num_tokens} tokens\")  \n",
    "\n",
    "  total_max_tokens = max_num_input_lines*10 + 30\n",
    "\n",
    "    # Loop through each file and calculate number of tokens\n",
    "  for file_name in file_names:\n",
    "      if file_name.endswith(\".txt\"):\n",
    "          with open(os.path.join(folder_path, file_name), \"r\" , encoding='utf-8') as f:\n",
    "              file_contents = f.read()\n",
    "          num_tokens = len(tokenizer.encode(file_contents))\n",
    "          total = num_tokens+ total_max_tokens\n",
    "          print(f\"{file_name}: Input: {num_tokens} tokens, Output: {total_max_tokens} : Total: {total}\")\n",
    "  print(total_max_tokens)\n",
    "  return total_max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c19d7f1-d671-457d-b459-391f9935c9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ChatGPT Completion\n",
    "def get_completion(BACKOFF_OCCURRED_, model, query, Sentences, max_tokens,  temperature): \n",
    "    prompt = f\"\"\"\n",
    "        {query}\n",
    "        {Sentences}\n",
    "        \"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    tries = 0\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "            )\n",
    "            call_time = time.time()\n",
    "            time_f = call_time - start_time\n",
    "            break\n",
    "        except (openai.error.RateLimitError, openai.error.ServiceUnavailableError, openai.error.APIError) as e:\n",
    "            tries += 1\n",
    "            max_backoff = 60 \n",
    "            backoff_time =  min(5 + 5*(tries ** 2), max_backoff)\n",
    "            with BACKOFF_OCCURRED_.get_lock():\n",
    "                BACKOFF_OCCURRED_.value = True\n",
    "            time.sleep(backoff_time)\n",
    "            with BACKOFF_OCCURRED_.get_lock():\n",
    "                BACKOFF_OCCURRED_.value = False\n",
    "            print(\"Backoff Released\\n\")\n",
    "    \n",
    "    message = response['choices'][0]['message']['content']\n",
    "    output_token = response['usage']['completion_tokens']\n",
    "    input_token = response['usage']['prompt_tokens']\n",
    "    \n",
    "    now_utc = datetime.datetime.now(pytz.utc)\n",
    "    timezone = pytz.timezone(\"US/Central\")\n",
    "    now_eastern = now_utc.astimezone(timezone)\n",
    "    time_stamp = str(now_eastern)\n",
    "    return message, input_token, output_token, time_f, time_stamp, tries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2dcec-d865-47ca-a8ab-bda2ce8a0919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "def call_ChatGPT(BACKOFF_OCCURRED_, current_folds, Implementation_base_path_input, Implementation_base_path_output, Run, total_max_tokens, temperature, query, dataset, model):\n",
    "        RETRY_COUNT = 5\n",
    "        input_file_path = os.path.join(Implementation_base_path_input, f'{current_folds}')\n",
    "        output_file_path = os.path.join(Implementation_base_path_output, f'{Run}_{current_folds}')\n",
    "        time_track_path = os.path.join(Implementation_base_path_output, f'{temperature}_time_track.csv')\n",
    "\n",
    "        while BACKOFF_OCCURRED_.value:\n",
    "            print(\"\\nBackoff occurred! Pausing all threads for a set duration...\")\n",
    "            sleep_time = random.randint(1, 5)\n",
    "            time.sleep(sleep_time)   \n",
    "            \n",
    "        for attempt in range(RETRY_COUNT):\n",
    "            try:\n",
    "                with open(input_file_path) as f:\n",
    "                    Sentences = f.read()\n",
    "                message, input_token, output_token, time_f, time_stamp, tries = get_completion(BACKOFF_OCCURRED_, model, query, Sentences=Sentences, max_tokens=total_max_tokens, temperature=temperature)\n",
    "\n",
    "                last_line = message.strip().split('\\n')[-1]\n",
    "                status = \"Complete\" if \"Done\" in last_line else \"Possibly Incomplete\"\n",
    "\n",
    "                print(f\"Fold ={current_folds}, Run= {Run}, Temperature={temperature}, {status}. output_file_path: {output_file_path}\")\n",
    "\n",
    "                with open(time_track_path, \"a\") as f:\n",
    "                    print(dataset, ',', Run, ',', current_folds, ',', temperature, ',', input_token, ',', output_token, ',', time_f, ',', time_stamp, ',', tries, file=f)\n",
    "\n",
    "                with open(output_file_path, \"w\") as f:\n",
    "                    print(message, file=f)\n",
    "                    \n",
    "                break\n",
    "                \n",
    "            except Exception as e:  # Catch general exceptions. Be specific if you know which exceptions to expect\n",
    "                    print(f\"Error occurred: {e}. Retrying {attempt+1}/{RETRY_COUNT}. output_file_path: {output_file_path}\")\n",
    "                    time.sleep(1) \n",
    "\n",
    "            else:  # This block will be executed if the for loop completed without 'break', i.e., if all attempts failed.\n",
    "                print(f\"All {RETRY_COUNT} retries failed for fold {current_folds} at Run {Run}. output_file_path: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc20d4c8-87a1-4d09-8e85-83a2428124b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "# Setting up the shared variable\n",
    "BACKOFF_OCCURRED_ = multiprocessing.Value('b', False)  # 'b' denotes a boolean\n",
    "\n",
    "def execute_code(run_no, prompt_no, model_engine, dataset, current_folds):\n",
    "    global BACKOFF_OCCURRED_\n",
    "    prompt_path = f\"Prompts/PROTEIN_Prompts/{prompt_no}.txt\"\n",
    "    temperature = 0.0 \n",
    "    query = user_input(prompt_path)\n",
    "    \n",
    "    Implementation_base_path_input = 'Datasets/PROTEIN_DATA/' + dataset + '/PROTEIN_splitted_all_sentences/input/'\n",
    "    print(Implementation_base_path_input)\n",
    "    Implementation_base_path_output = output_path(run_no, dataset, temperature, prompt_no, model_engine)\n",
    "    total_max_tokens = count_input_tokens(Implementation_base_path_input)\n",
    "    call_ChatGPT(BACKOFF_OCCURRED_, current_folds, Implementation_base_path_input, Implementation_base_path_output, run_no, total_max_tokens, temperature, query, dataset,model = model_engine)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    prompt_no = ['Prompt15']\n",
    "    model_engines = [\"gpt-3.5-turbo-0613\", \"gpt-4-0613\"]\n",
    "    datasets = [\"LLL\", \"HPRD50\", \"IEPA\"]\n",
    "    \n",
    "    # LLL - Folds 4 and 9 have part1 and part2\n",
    "    folds_LLL = [f\"fold{i}.txt\" for i in range(1, 11) if i not in [4, 9]]\n",
    "    folds_LLL.extend([f\"part{j}_fold4.txt\" for j in range(1, 3)])\n",
    "    folds_LLL.extend([f\"part{j}_fold9.txt\" for j in range(1, 3)])\n",
    "\n",
    "    # HPRD50 - Fold 4 has part1 and part2\n",
    "    folds_HPRD50 = [f\"fold{i}.txt\" for i in range(1, 11) if i != 4]\n",
    "    folds_HPRD50.extend([f\"part{j}_fold4.txt\" for j in range(1, 3)])\n",
    "\n",
    "    # IEPA - All folds have part1 and part2, fold 1 additionally has part3\n",
    "    folds_IEPA = [f\"part{j}_fold{i}.txt\" for i in range(1, 11) for j in range(1, 3)]\n",
    "    folds_IEPA.extend([\"part3_fold1.txt\"])\n",
    "\n",
    "\n",
    "    for Run_no in range(1, 11):\n",
    "        # Get all combinations\n",
    "        all_combinations = get_combinations(prompt_no, model_engines, datasets, folds_LLL, folds_HPRD50, folds_IEPA)\n",
    "        # Print the total number of combinations\n",
    "        print(f\"Total number of combinations: {len(all_combinations)}\")\n",
    "\n",
    "        # Create argument tuples for starmap\n",
    "        args = [(Run_no,) + combo for combo in all_combinations]\n",
    "        print(\"\\n\\n\\n\", args)\n",
    "        # Use as many workers as there are CPUs available\n",
    "        no_of_workers = 20\n",
    "\n",
    "        with multiprocessing.Pool(no_of_workers) as pool:\n",
    "            pool.starmap(execute_code, args)\n",
    "            time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openaienv",
   "language": "python",
   "name": "openaienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
