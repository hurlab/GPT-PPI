{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2252a90-a283-4858-b96a-4ed82d4c36bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import io\n",
    "import openai\n",
    "from transformers.models.imagegpt.modeling_imagegpt import IMAGEGPT_INPUTS_DOCSTRING\n",
    "from transformers import GPT2Tokenizer\n",
    "import pandas as pd \n",
    "from pandas.io import json\n",
    "from numpy import nan\n",
    "import time\n",
    "import csv\n",
    "import shutil\n",
    "import datetime\n",
    "import pytz\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import random\n",
    "import itertools\n",
    "import random\n",
    "import multiprocessing\n",
    "import time\n",
    "import os\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04eb7eb-ddb5-4bef-832b-44ea30e234c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300d5eaa-1994-4e3f-834f-5362c7a5dc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"OPENAI_API_KEY\"\n",
    "section_no = \"PROTEIN_Prompts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d624a146-0946-4138-8322-97ca1108cb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate All the variations \n",
    "def get_combinations(prompt_paths, model_engines, datasets, folds_LLL, folds_HPRD50, folds_IEPA):\n",
    "    combinations = []\n",
    "    # Create dictionary for dataset-specific folds\n",
    "    dataset_folds = {\n",
    "        'LLL': folds_LLL,\n",
    "        'HPRD50': folds_HPRD50,\n",
    "        'IEPA': folds_IEPA\n",
    "    }\n",
    "    # Iterate over each dataset to get the dataset-specific folds and prompts\n",
    "    for dataset in datasets:\n",
    "        current_prompts = prompt_paths\n",
    "        current_folds = dataset_folds[dataset]\n",
    "        for combination in itertools.product(current_prompts, model_engines, [dataset], current_folds):\n",
    "            combinations.append(combination)\n",
    "    random.shuffle(combinations)\n",
    "    return combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd39f5f-f18a-401f-ae76-9ee8f42a60d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_input(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            query = file.read()\n",
    "            file_name = file_path.split('/')[-1]  # Extract the file name from the path\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{file_path} not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cada087-e939-4710-9b52-634e888dc524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def output_path(Run_no, dataset, temperature, prompt_no, model_engine):\n",
    "    base = \"Output/\" + model_engine + \"_OneSentence_\"+section_no+\"_\" + str(temperature) + \"/\" + str(prompt_no) +\"/\"+dataset + \"/\"\n",
    "    extension_path = dataset + \"_T\" + str(temperature) + \"_\" + prompt_no + \"_Run\" + str(Run_no)+'/'\n",
    "    Implementation_base_path_output = os.path.join(base, extension_path)\n",
    "    os.makedirs(Implementation_base_path_output, exist_ok=True)\n",
    "    return Implementation_base_path_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340018c4-39ff-474b-86fe-5c9f99556ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_input_tokens(folder_path):\n",
    "  tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "  file_names = os.listdir(folder_path)\n",
    "  max_num_input_lines = 0\n",
    "\n",
    "  # Loop through each file and calculate number of tokens\n",
    "  for file_name in file_names:\n",
    "      # Check if file is a text file\n",
    "      if file_name.endswith(\".txt\"):\n",
    "          # Read file contents\n",
    "          with open(os.path.join(folder_path, file_name), \"r\" , encoding='utf-8') as f:\n",
    "              file_contents = f.read()\n",
    "          with open(os.path.join(folder_path, file_name), \"r\" , encoding='utf-8') as fp:\n",
    "              num_input_line = len(fp.readlines())\n",
    "              print(\"num_input_line:\",num_input_line)\n",
    "          # Calculate number of tokens\n",
    "          num_tokens = len(tokenizer.encode(file_contents))\n",
    "          if (num_input_line>max_num_input_lines):\n",
    "            max_num_input_lines = num_input_line\n",
    "          print(f\"{file_name}: {num_tokens} tokens\")\n",
    "    \n",
    "  total_max_tokens = 10\n",
    "\n",
    "  # Loop through each file and calculate number of tokens\n",
    "  for file_name in file_names:\n",
    "      # Check if file is a text file\n",
    "      if file_name.endswith(\".txt\"):\n",
    "          # Read file contents\n",
    "          with open(os.path.join(folder_path, file_name), \"r\" , encoding='utf-8') as f:\n",
    "              file_contents = f.read()\n",
    "          # Calculate number of tokens\n",
    "          num_tokens = len(tokenizer.encode(file_contents))\n",
    "          total = num_tokens+ total_max_tokens\n",
    "          print(f\"{file_name}: Input: {num_tokens} tokens, Output: {total_max_tokens} : Total: {total}\")\n",
    "  print(total_max_tokens)\n",
    "  return total_max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f936e3b-af98-4822-9187-dcd16a9b7458",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ChatGPT Completion\n",
    "def get_completion(BACKOFF_OCCURRED_, model, query, Sentences, max_tokens,  temperature):\n",
    "    prompt = f\"\"\"\n",
    "        {query}\n",
    "        {Sentences}\n",
    "        \"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    tries = 0\n",
    "    while True:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "            )\n",
    "            call_time = time.time()\n",
    "            time_f = call_time - start_time\n",
    "            break\n",
    "        except (openai.error.RateLimitError, openai.error.ServiceUnavailableError, openai.error.APIError) as e:\n",
    "            tries += 1\n",
    "            max_backoff = 60  # Example: maximum of 60 seconds\n",
    "            backoff_time =  min(5 + 5*(tries ** 2), max_backoff)\n",
    "            with BACKOFF_OCCURRED_.get_lock():\n",
    "                BACKOFF_OCCURRED_.value = True\n",
    "            time.sleep(backoff_time)\n",
    "            with BACKOFF_OCCURRED_.get_lock():\n",
    "                BACKOFF_OCCURRED_.value = False\n",
    "            print(\"Backoff Released\\n\")\n",
    "    \n",
    "    message = response['choices'][0]['message']['content']\n",
    "    output_token = response['usage']['completion_tokens']\n",
    "    input_token = response['usage']['prompt_tokens']\n",
    "    now_utc = datetime.datetime.now(pytz.utc)\n",
    "    timezone = pytz.timezone(\"US/Central\")\n",
    "    now_eastern = now_utc.astimezone(timezone)\n",
    "    time_stamp = str(now_eastern)\n",
    "    return message, input_token, output_token, time_f, time_stamp, tries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b736341a-f203-4a00-8bba-548cb3bd6a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_ChatGPT(BACKOFF_OCCURRED_, current_folds, Implementation_base_path_input, Implementation_base_path_output, Run, total_max_tokens, temperature, query, dataset, model):\n",
    "    RETRY_COUNT = 5\n",
    "    input_file_path = os.path.join(Implementation_base_path_input, f'{current_folds}')\n",
    "    output_file_path = os.path.join(Implementation_base_path_output, f'{Run}_{current_folds}')\n",
    "    time_track_path = os.path.join(Implementation_base_path_output, f'{temperature}_time_track.csv')\n",
    "\n",
    "    while BACKOFF_OCCURRED_.value:\n",
    "        print(\"\\nBackoff occurred! Pausing all threads for a set duration...\")\n",
    "        sleep_time = random.randint(1, 5)\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "    with open(input_file_path) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Write header to the output file\n",
    "    with open(output_file_path, \"a\") as f:\n",
    "        print(\"Sentence ID,PPI\", file=f)\n",
    "\n",
    "    for line_id, line in enumerate(lines, start=1):\n",
    "\n",
    "        line = line.strip()\n",
    "        if line.startswith('Sentence ID'):\n",
    "            continue \n",
    "\n",
    "        sentence_id, sentence_rest = line.split('\\t', 1)\n",
    "\n",
    "        for attempt in range(RETRY_COUNT):\n",
    "            try:\n",
    "                message, input_token, output_token, time_f, time_stamp, tries = get_completion(BACKOFF_OCCURRED_, model, query, Sentences=sentence_rest, max_tokens=total_max_tokens, temperature=temperature)\n",
    "\n",
    "                print(f\"Sentence ID={sentence_id}, Fold={current_folds}, Run={Run}, Temperature={temperature}.\")\n",
    "\n",
    "                with open(time_track_path, \"a\") as f:\n",
    "                    print(dataset, ',', Run, ',', current_folds, ',', sentence_id, ',', temperature, ',', input_token, ',', output_token, ',', time_f, ',', time_stamp, ',', tries, file=f)\n",
    "\n",
    "                with open(output_file_path, \"a\") as f:\n",
    "                    print(f\"{sentence_id},{message}\", file=f)\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error occurred: {e}. Retrying {attempt+1}/{RETRY_COUNT} for Sentence ID={sentence_id}.\")\n",
    "                time.sleep(1) \n",
    "\n",
    "            if attempt == RETRY_COUNT - 1:\n",
    "                print(f\"All {RETRY_COUNT} retries failed for Sentence ID {sentence_id} at Fold {current_folds}, Run {Run}.\")\n",
    "        if line_id % 10 == 0:\n",
    "            print(\"Pausing for a short break...\")\n",
    "            time.sleep(1)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647da7df-eea7-46c7-a92e-d0469092dd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKOFF_OCCURRED_ = multiprocessing.Value('b', False)  # 'b' denotes a boolean\n",
    "\n",
    "def execute_code(run_no, prompt_no, model_engine, dataset, current_folds):\n",
    "    global BACKOFF_OCCURRED_\n",
    "    prompt_path = f\"Prompts/PROTEIN_Prompts/{prompt_no}.txt\"\n",
    "    temperature = 0.0 \n",
    "    Implementation_base_path_input = 'Datasets/PROTEIN_DATA/' + dataset + '/PROTEIN_all/input/'\n",
    "    Implementation_base_path_output = output_path(run_no, dataset, temperature, prompt_no, model_engine)\n",
    "    total_max_tokens = count_input_tokens(Implementation_base_path_input)\n",
    "    call_ChatGPT(BACKOFF_OCCURRED_, current_folds, Implementation_base_path_input, Implementation_base_path_output, run_no, total_max_tokens, temperature, query, dataset,model = model_engine)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    prompt_no = ['Prompt15_OneSentence']\n",
    "    model_engines = [\"gpt-3.5-turbo-0613\", \"gpt-4-0613\"]\n",
    "    datasets = [\"LLL\", \"IEPA\", \"HPRD50\"]\n",
    "    \n",
    "    # List for LLL, IEPA and HPRD50 with 10 folds\n",
    "    folds_LLL = [f\"fold{i}.txt\" for i in range(1, 11)]\n",
    "    folds_HPRD50 = [f\"fold{i}.txt\" for i in range(1, 11)]\n",
    "    folds_IEPA = [f\"fold{i}.txt\" for i in range(1, 11)]\n",
    "\n",
    "    for Run_no in range(2, 11):\n",
    "        all_combinations = get_combinations(prompt_no, model_engines, datasets, folds_LLL, folds_HPRD50, folds_IEPA)\n",
    "        print(f\"Total number of combinations: {len(all_combinations)}\")\n",
    "        args = [(Run_no,) + combo for combo in all_combinations]\n",
    "        no_of_workers = 5\n",
    "        with multiprocessing.Pool(no_of_workers) as pool:\n",
    "            pool.starmap(execute_code, args)\n",
    "            time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openaienv",
   "language": "python",
   "name": "openaienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
